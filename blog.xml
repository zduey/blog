<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Zach Duey</title>
<link>https://www.zduey.github.io/blog.html</link>
<atom:link href="https://www.zduey.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Personal website for Zach Duey</description>
<generator>quarto-1.4.552</generator>
<lastBuildDate>Tue, 26 Mar 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Quantifying Uncertainty Using the Nonparametric Bootstrap</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/applying-nonparametric-bootstrap.html</link>
  <description><![CDATA[ 





<p>I love the bootstrap. More specifically, I love the non-parametric bootstrap. I may love it too much. It is the most commonly used non-standard tool in my applied statistics toolbox. In this post, I’ll briefly explain the bootstrap procedure and how I have used it to quantify uncertainty in various real-world problems. This post is neither a tutorial on the procedure nor an exhaustive list of use cases. My goal is simple: convince you that the bootstrap is a tool you should have in your toolbox by describing some real-world applications.</p>
<section id="background" class="level1">
<h1>Background</h1>
<p>The bootstrap is a procedure introduced in <a href="https://www.jstor.org/stable/2958830">Efron (1976)</a> for estimating the sampling distribution of a random variable by repeatedly sampling with replacement from available data. The beauty of the procedure lies in its broad applicability and surprising simplicity.</p>
<p>The bootstrap procedure is widely applicable for practitioners because it can quantify uncertainty. Uncertainty quantification is a broad topic well beyond the scope of this post, so we will start by honing in on how the bootstrap procedure can solve a central problem in frequentist statistics: estimating the sampling distribution of a random variable. In textbook examples, sampling distributions often follow well-known probability distributions (e.g., Gaussian, Student’s t, etc.). However, the sampling distributions encountered in practice can be cumbersome to identify or challenging to derive. Therefore, a procedure that allows practitioners to approximate virtually any sampling distribution is immensely valuable.</p>
<p>The bootstrap procedure’s conceptual simplicity is due to its taking the ideas underlying statistical inference to their logical extreme. If you understand statistical inference, then there is no additional conceptual burden to understanding the bootstrap. The bootstrap is even sometimes used to teach statistical inference.</p>
<p>Statistical inference is the process of using a sample of data to learn something about the population from which the sample originated. Inference starts with defining a statistic, a measure computed from the sample data. This statistic captures something of interest about the population. For example, you may want to know about the birth weight of babies born in the United States. Since it would be prohibitive to get accurate birth weight measurements for all babies born in the US, we may choose to collect measurements for a sample of babies. We compute the average birth weight within this sample to learn something about the average birth weight in the population. In this example, average birth weight is the statistic. Since our sample is just one of many possible samples, any statistic we compute is a random variable. Therefore, to make inferences about the population, we need to pin down some properties of this random variable. The fundamental challenge is that we only have access to a single sample. However, we can imagine running this same experiment many times. This hypothetical resampling leads to the idea of a sampling distribution. Depending on how the statistic is defined and other assumptions we make, we can often derive properties of this sampling distribution that support inferential tasks. This idea of resampling is the bridge between statistical inference and the bootstrap, which we can now discuss.</p>
<p>The bootstrap procedure come from a straightforward idea: resample from the sample data as if the sample is the full population. This technique allows us to approximate the sampling distribution directly instead of deriving its properties via a thought experiment. As long as the sample is reasonably large and representative of the population, the approximation should be good enough. The simplest version of the procedure is just five steps:</p>
<ol type="1">
<li>Define a statistic of interest</li>
<li>Sample with replacement from the data</li>
<li>Compute the statistic from (1) using the bootstrap sample (2)</li>
<li>Repeat steps 2 and 3 many times</li>
<li>Compute whatever you want from the results of 2-4</li>
</ol>
<p>The canonical use case for the bootstrap is estimating standard errors. A standard error is the standard deviation of a sampling distribution. In our birth weight example, the standard error quantifies the uncertainty associated with using the average birth weight in our sample to approximate the fixed but unknown average birth weight in the unobserved population. Since this is the application that most people are familiar with, we will focus most of our attention on some other, less common applications involving predictive modeling and related decision problems. Let’s dive in!</p>
</section>
<section id="applications" class="level1">
<h1>Applications</h1>
<p>In the remainder of this post, I will outline three additional ways I have used the bootstrap procedure. To keep the discussion grounded, I will use a case study that closely mirrors an actual project I have worked on. Many of these applications build on one another, so I do not recommend skipping ahead. While far from exhaustive, these examples will ideally spark some ideas about how you could apply the procedure.</p>
<p>Imagine that you asked to improve the efficiency of your employer’s customer acquisition process. The company has a list of prospective customers. Each day, this list is handed off to a call center. The call center representatives attempt to contact each individual on the list from top to bottom. The list is long enough that it takes many days to multiple weeks to contact everyone. If the represented cannot reach the prospective customer, the customer is removed from the call list. If the representative is able to reach the prospective customer, the representative asks them a few questions to determine if they are eligible to receive your company’s services. Some of these individuals will not be eligible. Some eligible individuals will opt out of receiving services. Notably, the company can only provide services to a limited number of individuals. Your employer maximizes revenue when operating at capacity and revenue quickly declines if over or under-utilized. Your task is to help maximize revenue, but the only lever you have available is the order of the individuals on the contact list. There are several ways to solve this problem, but we will focus on a predictive model-based solution.</p>
<p>Let’s assume that you have decided to develop a binary classifier that predicts if a prospective customer is eligible to receive services. Although eligibility does not guarantee that the individual will elect to receive services, it is a prerequisite. You plan to use the predicted probabilities generated by the model to re-order the list so that the most likely to be eligible individuals are at the top. If the model is effective, then the company should be able to achieve higher revenue.</p>
<section id="how-well-will-this-model-perform-on-unseen-data" class="level2">
<h2 class="anchored" data-anchor-id="how-well-will-this-model-perform-on-unseen-data">How well will this model perform on unseen data?</h2>
<p>An essential task when developing a predictive model is understanding how well it is likely to perform on unseen data; in other words, estimating the model’s expected generalization error. The bootstrap procedure can quantify the uncertainty associated with this measure. Before delving into the specifics of this technique, it is valuable to explore a common approach to model evaluation. These details will be important later in our discussion.</p>
<p>At the start of the model development process, you partition the available data into multiple subsets. Whether you choose to do two or three partitions is irrelevant; the result is that you hold out some of the data until after model comparison and selection are complete. You can then use the held-out data to estimate expected generalization error.</p>
<p>You may be wondering why we need the bootstrap procedure. Why can’t we just calculate model performance on the held-out data? We can! But, the key thing to realize is that model performance computed on the held-out data is not a fixed quantity but a random variable. It is a random variable for the same reason that any statistic computed on a sample of data is a random variable; the observed sample, in this case, the data available for training, is just one of many possible samples. As with traditional statistical inference, we would like to estimate the sampling distribution of this statistic, which is exactly what the bootstrap procedure can do. In essence, it is a way to quantify the uncertainty with our measure of expected generalization error. The core of the procedure is just four steps:</p>
<ol type="1">
<li>Select a model performance metric</li>
<li>Draw a sample with replacement from the held-out data</li>
<li>Compute (1) using the data from (2)</li>
<li>Repeat 2-3 many times</li>
</ol>
<p>At the end of this process, we have a measure of out-of-sample performance across many bootstrap samples. We can use this data in a few ways. First, we can plot a histogram of these performance metrics to visualize the empirical sampling distribution. Second, if this sampling distribution is approximately normal, we can generate confidence intervals for the performance metric using the 5th and 95th percentiles of the performance metric across our bootstrap samples. Third, we can compute the standard deviation of these values to estimate the standard error for the expected generalization error. We could do other things, but these are at least some of the main ones.</p>
<p>Astute readers will have noticed something different about this bootstrap application relative to traditional inference. In this application, we have two sources of uncertainty. We have the typical source of uncertainty arising from the sample data being just one of infinitely many possible samples we could have observed. However, we also have a second source of uncertainty due to the partitioning performed at the start of the model development process. If the randomization had been different (e.g., using a different seed), these partitions would have contained different observations, which impacts all downstream model development steps.</p>
<p>The bootstrap procedure described above does not account for the uncertainty due to random partitioning, so it underestimates the overall uncertainty associated with our estimate of expected generalization error. We could account for this variation by wrapping our data splitting, data processing, model training, evaluation, and selection pipeline into a bootstrap procedure. In the end, the confidence intervals for our performance metric account for both sources of uncertainty. While this nested bootstrap procedure is undoubtedly a better way to capture the relevant uncertainty, I don’t typically go this extra step. The main reason is practical; for anything beyond a toy dataset, the computational cost and complications of bootstrapping the whole model development process outweigh the benefits of better-capturing uncertainty.</p>
</section>
<section id="which-model-is-better" class="level2">
<h2 class="anchored" data-anchor-id="which-model-is-better">Which model is better?</h2>
<p>In our second non-standard bootstrap application, we will take a step back in the model development lifecycle and to use it for model comparison. The idea is simple: if you can use the bootstrap procedure to better understand a single model’s performance, why not use it to compare models?</p>
<p>The bootstrap procedure for model comparison is nearly identical to the one for model evaluation, with a few slight tweaks. First, you should use either the validation or training data. If you created three partitions, then you will use the validation data. Otherwise, you will use the training data. The held-out data should be reserved until you are ready to decide about moving the selected model to production. Second, you must evaluate the models on identical bootstrap samples. Resist the temptation to generate independent bootstrap samples and evalute the models separately. Unless you are exceptionally careful with setting up the seeds, the bootstrap samples will be different, making the comparison invalid. Third, you must train both models on the same data.</p>
<p>As a practical matter, I find it helpful to designate the more complex model as the “challenger” and the simpler one as the “benchmark.” While there is no perfect way to measure model complexity, a general gut sense based on model flexibility, number of hyperparameters, and number of input features is sufficient. By framing model comparison this way, simpler models are the default preference. The actual process is just five steps:</p>
<ol type="1">
<li>Select a performance metric</li>
<li>Draw a sample with replacement (validation data if available otherwise training)</li>
<li>Compute performance for each model using data from (2)</li>
<li>Compute the difference in performance between the models using (3)</li>
<li>Repeat 2-4 many times</li>
</ol>
<p>At the end of this procedure, there is a sequence of model performance metrics for each model as well as a third sequence that captures the difference between them. We can plot a histogram of these metrics and their difference. I typically overlay a vertical line at zero on top of the performance difference histogram. If you or your colleagues insist on turning this data into a hypothesis test, it is straightforward. Assume that you are interested in the null hypothesis that the benchmark model performs equally well as the challenger. The test statistic is the number of times that the difference between the challenger and benchmark was greater than zero. You can get a p-value by dividing this test statistic by the number of bootstrap samples. It is up to you if and how you want to use this p-value. I dare not trudge into that debate, but suffice it to say that I do not bother looking at p-values. Instead, I find it is sufficiently informative to look at the histogram of the difference in performance. Ultimately, the decision about whether the challenger is sufficiently better than the benchmark is a decision-theoretic one that is highly context-dependent. We will delve a little deeper into that realm with our next application.</p>
</section>
<section id="should-i-deploy-this-model" class="level2">
<h2 class="anchored" data-anchor-id="should-i-deploy-this-model">Should I deploy this model?</h2>
<p>Our third application uses the bootstrap procedure to quantify the uncertainty associated with estimating the expected value to the business of deploying the model to production. This application builds on the first, where we used the bootstrap to quantify the uncertainty associated with expected generalization error. It also builds on the second application in that you evaluate the the expected value of the model relative to the current solution, which is essential model comparison. The current solution could be a rules-based algorithm, predictive model, or anything else, so long as you can use it to derive comparable performance metrics.</p>
<p>We have now moved firmly away from traditional predictive modeling and evaluation into the realm of decision science. At a high level, there are two steps. First, a variant of the model comparison bootstrap technique discussed previously is be used to compare the candidate (challenger) solution to the current solution (benchmark). Second, those performance differences are converted into a dollar value. The overall procedure is again just five steps:</p>
<ol type="1">
<li>Specify the costs and benefits of each prediction outcome</li>
<li>Draw a sample with replacement from the held-out data</li>
<li>Compute the difference in performance between the challenger and benchmark</li>
<li>Compute the net benefit of the challenger relative to the current solution</li>
<li>Repeat 2-4 many times</li>
</ol>
<p>Although we are using the procedure to compare two models as in the previous application, we are not using the results for model selection, which means we can use the held-out data. The other reason to use the held-out data is that it provides a fairer comparison since the current solution was likely trained different data than he challenger. This way, the evaluation is out of sample for both the benchmark and challenger.</p>
<p>The second step requires aome additional assumptions. Remember that in our running example, we are evaluating a binary classifier, so to estimate the expected value of deploying this model, we need to assign a dollar value to each of the four prediction outcomes: true positives, false positives, true negatives, and false negatives. Then, the math is pretty simple: take a weighted sum of the elements of the confusion matrix and their associated value. Do this for both the challenger and current solution and take their difference to arrive at the added value of the challenger solution. We can leverage these results across the bootstrap samples to get a sense of the uncertainty of our estimate. As a practical matter, because this evaluation requires some additional assumptions, I usually perform a sensitivity analysis. This sensitivity analysis requires repeating the evaluation over a range of plausible parameter values (cost/benefit assumptions).</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this post, we looked at three ways to apply the nonparametric bootstrap in the model development process. These three applications tried to address the following three questions:</p>
<ol type="1">
<li>How well will this model perform on unseen data?</li>
<li>Which model is better?</li>
<li>Should I deploy this model?</li>
</ol>
<p>One theme underlying this discussion is that the nonparametric bootstrap is useful for quantifying uncertainty. Most commonly, the procedure is used to estimate the standard deviation of a sampling distribution, which is just a way of quantifying uncertainty associated with a parameter estimate (e.g., average birth weight in the US). Our first application used the procedure to quantify uncertainty when estimating expected generalization error. The second application used the bootstrap for model comparison: quantifying the uncertainty associated with estimating the difference in model performance. The final application quantified the uncertainty related to estimating the expected benefit of deploying a challenger model relative to the current solution.</p>
<p>Although the bootstrap procedure has limitations, uncertainty quantification is an area where it can shine. Incorporating these different sources of uncertainty means that the decision is not as clear-cut as standard techniques that generate point estimates. But that’s kind of the point, isn’t it? The model development lifecycle isn’t always straightforward and linear in the real world. In the face of uncertainty, we can ignore it or incorporate it into our decision-making. Hopefully, this post has given you some ideas about how to do the latter using the nonparametric bootstrap procedure!</p>


</section>

 ]]></description>
  <category>statistics</category>
  <category>machine learning</category>
  <guid>https://www.zduey.github.io/posts/applying-nonparametric-bootstrap.html</guid>
  <pubDate>Tue, 26 Mar 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Courage to Ask: A Resolution to Counteract Hyperpartisanship</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/courage-to-ask.html</link>
  <description><![CDATA[ 





<p>I originally published this essay on <a href="https://zduey.medium.com/courage-to-ask-99b325e85786">Medium</a>. While the framing (2021 resolutions) is now outdated, the issue remains as do the original conclusions.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>There is a problem with the state of political discourse in the United States. While intra-party discourse can and should be improved, what has me more concerned, what keeps me awake at night, and what should trouble us all is how the current level of political polarization has all but destroyed inter-party discourse. From the family dinner table to Washington, DC, productive political dialogue is rare, basic factual statements are not recognized as such, and, even in the face of compelling evidence, we are generally unwilling to change our minds. While I recognize that we are faced with many important issues today — from COVID-19 to climate change to racial and economic injustice — I am particularly concerned about hyperpartisanship because I believe it is a foundational issue that precludes effective solutions to other critically-important problems.</p>
<p>I expect that not everyone shares my view that hyperpartisanship is a foundational problem, however, I hope we can agree that it is a problem worthy of our attention. If we start on this common ground, then we can collectively address both hyperpartisanship and the many other challenges we face. I stress the collective aspect because I believe that hyperpartisanship neither originated with nor can be solved by a single group. Instead, I contend that political polarization arose organically from the information overload we experience as part of the modern world — a world defined by nearly instantaneous communication and an endless barrage of information. While I recognize that systemic forces also play an important role, fully addressing them is beyond the scope of this essay. Instead, I want to focus on what we, as individuals, can do to combat hyperpartisanship.<sup>1</sup></p>
<p>My proposed solution involves three steps. First, each of us must alleviate information overload by judiciously but dramatically restricting the information we access. Only then will we be in a position to effectively process new information. The second step is to engage in honest self-reflection about how we form opinions and then take corrective action to improve that process. These two changes will help us to take the third step, which is to approach others with empathy. In the sections that follow, I will explain why these three steps are necessary and how they can help us to move forward.</p>
<p>In the first section below, I explain how information overload has led to a self-reinforcing cycle of political polarization, culminating in the widespread hyperpartisanship we experience today. I suggest that this cycle exists because we often fail to think critically about the information we consume. Thus, we must define what it means to think critically. Because critical thinking is a nebulous term, I spend the majority of this essay exploring a framework taken from Bayesian statistics that guides us in this effort. Before I lose you, let me be clear that I am not claiming that statistics, in itself, is the solution. Instead, if we take an abstract, perhaps philosophical view of the principles underlying Bayesian statistics, we can repurpose them to develop a model of how we form opinions. This framework provides important guideposts for becoming better critical thinkers and, just as importantly, illuminates the multitude of ways in which individuals who are thinking critically can arrive at different conclusions. With a clear understanding of what we mean by critical thinking, in the final section I propose a plan that seeks to counteract political polarization by addressing its underlying causes.</p>
<p>I recognize that this essay is rather lengthy, so if you only have a few minutes, I encourage you to skip to the final section. There, I give concrete recommendations about what you can do to help counteract hyperpartisanship. In order to affect the kind of change that is required to make a dent in this issue, I need a critical mass of people willing to join me in this effort. In this time of reflection and planning for the new year, I challenge you to join me in adding “The Countdown” to your 2021 resolutions.</p>
</section>
<section id="information-overload-and-hyperpartisanship" class="level1">
<h1>Information Overload and Hyperpartisanship</h1>
<p>The full history of how hyperpartisanship developed in the United States is well beyond the scope of this essay. Yet, if we can identify at least one significant contributing factor, then we can start to address the problem by focusing our attention there. In this section, we will see how information overload causes all of us, to varying degrees, to rely on time-saving heuristics that perpetuate political polarization.</p>
<p>In order to have well-grounded opinions about a topic, we should aim to incorporate all relevant information. However, this ideal is unattainable because we have limited time to acquire and critically assess that information. To critically evaluate information from any given source, we must have access to it, have the expertise to evaluate it, and believe that the source is trustworthy. Since most people in the United States have nearly instantaneous access to historically-unprecedented amounts of information, “access” more precisely refers to what information we choose to access in our limited time. Because there are so many news sources, we all have our favorites; yet, these preferences can become problematic, given how news sources present the same information in different ways.<sup>2</sup> In order to critically evaluate information, we must also have some amount of background knowledge, or expertise. For example, the vast majority of us are not climate scientists, and therefore we cannot fully and independently evaluate the existing data about climate change and come to our own conclusions. As a result, we are forced to either develop that expertise, or to trust the evaluations provided by the news sources that we consume. Intuitively, our level of trust in a given source should correlate with the quality of the source’s evaluations. However, in order to do that, we need another source that is universally trusted to use as a benchmark, thereby leading us into a vicious cycle of seeking out new trusted sources to establish an appropriate level of trust in each preceding source. Alternatively, we can assess a source’s trustworthiness by looking at how that source presents information that we do have the expertise to independently evaluate. This approach effectively turns each of us into our own “universally-trusted” source, which leads to two problems. First, we do not always evaluate information correctly; second, we are rarely willing to accept others as universally-trusted sources, so it is unreasonable to assume that we will be accepted as such.</p>
<p>The components required to critically assess new information — access, expertise, and trust — are affected by the amount of time we have available. Because time is a limited resource, it also limits our ability to think critically about important issues; we move further away from the ideal of incorporating all available information the more time constrained we are. While time has always been a factor, it has become a limiting factor as the amount of available information has exploded. In response to this information overload, we are forced to resort to time-saving heuristics. In the political realm, we often adopt the view held by the political party that most closely aligns with our beliefs. We also selectively consume information from sources that reinforce our beliefs. Any time we engage one of these heuristics, we fail to think critically because we are effectively ignoring new information. As a result, we become dangerously overconfident in our views, incorrectly assuming that we have critically considered the information that informs them.</p>
<p>Information overload alone is insufficient to cause hyperpartisanship. Yet when combined with a small number of dominant political ideologies and biased sources of information, it leads to political polarization and ultimately hyperpartisanship. In the United States, we have exactly this set of conditions. The two-party system gives us a limited number of divergent political ideologies; both parties must differentiate themselves from one another in order to expand their respective bases. In addition, the majority of our news sources lean towards one or the other of these political ideologies. As time-constrained individuals, we all use one or both of the time-saving heuristics mentioned earlier at least some of the time, causing us to drift towards one of these two ideological extremes. This ideological drift underlies political polarization and, because it is rooted in information overload, is a self-sustaining process so long as information overload remains a problem.</p>
<p>The solution to hyperpartisanship hinges on being able to break out of this vicious cycle. As individuals, we must find an alternative “heuristic” that alleviates information overload but does not cause us to ignore information. One approach is to restrict the number of topics towards which we direct our attention. Even though we still have a limited amount of time, we can at least incorporate the information related to this restricted set of topics. In the final section of this essay, I provide some additional details about a resolution based on this approach. However, we must first address the glaring problem with this analysis; I have been discussing the importance of “critically evaluating” information but have not provided any details about how to do so. We will arrive at these details in an unexpected way, by delving into Bayesian statistics.</p>
</section>
<section id="bayesian-statistics" class="level1">
<h1>Bayesian Statistics</h1>
<p>In order to understand how I arrived at my proposed plan for counteracting hyperpartisanship and why I think it is a viable solution, we must first review the key components of Bayesian statistics.<sup>3</sup> I suspect that most of us have encountered word problems like this one: “There is a bag with 1 blue marble and 3 white marbles. Jill reaches into the bag and pulls out 1 marble. What is the probability that Jill selects a blue marble?” The problem tells us the number of marbles of each color in the bag, so the probability that Jill selects a blue marble is 1/4 or 25%. Now consider a different scenario in which we are told that the bag contains 4 marbles that are either blue or white. In this scenario, we are tasked with determining the number of marbles of each color in the bag, and we are allowed to do so by picking one marble from the bag at a time, observing its color, and putting it back. Without picking any marbles, we already know that the bag must contain one of the following five combinations: 4 blue, 3 blue and 1 white, 2 blue and 2 white, 1 blue and 3 white, or 4 white. Because the bag’s contents are unknown, we refer to each of these possibilities as a conjecture. In this new problem, our goal is to determine which of these conjectures is the most plausible.</p>
<p>In order to determine the most plausible conjecture, we need data—the first key component of Bayesian statistics. We collect data by following the procedure outlined previously: select a marble, observe its color, put it back, and repeat. To solve the problem, we count all of the ways that our observed sequence of picked marbles (our data) could have occurred if we assume, in turn, that each one of the five conjectures were true. For example, assume that we have observed two blue marbles and one white marble as our data. If we start with the conjecture that the bag contains all white marbles, then we can say that there are zero ways that this sequence could have occurred. We then repeat this exercise for the remaining four conjectures, and the conjecture with the most possibilities is the most plausible. This approach of using data to learn about something unknown is a core idea in Bayesian statistics.</p>
<p>To make things clearer, we can take this example further. While we are allowed to repeat the marble-picking process any number of times, assume that we only do it three times. Since the marbles are replaced each time and there are four of them, we know that there are 4 x 4 x 4 = 64 possible sequences. In general, if we repeat the marble-picking process n times, there are 4n possible sequences. Here, we must introduce the second component of Bayesian statistics: the prior distribution (prior). The prior is how we encode our preexisting knowledge about how likely each possible bag configuration is before, or prior to, observing any data. We do not have any such prior information in our scenario, so we should consider each conjecture to be equally likely. This type of prior is called a flat prior, a term we will explore later. As an additional note, we can imagine a variation of this scenario in which we do have prior information. For example, if we are told that the marble bag manufacturer produces twice as many bags with equal numbers of blue and white marbles as every other kind of bag, then it no longer makes sense to treat each bag configuration as equally likely. Again, we will explore this idea later in the essay. For now, we will consider the scenario as originally stated, with no prior information.</p>
<p>After we encode our belief that each bag configuration is equally likely, we will update this belief based on the data. Assume again that when we were picking marbles, we selected two blue marbles, followed by a white marble. Our goal is to count how many times this particular sequence (blue, blue, white) could have occurred for each of the five possible bag configurations. Once we have this information, we can rank the possible bag configurations from least plausible to most plausible. From there, we can make informed statements about the contents of the bag.</p>
<table class="table">
<colgroup>
<col style="width: 35%">
<col style="width: 30%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Bag (Conjecture)</strong></th>
<th style="text-align: center;"><strong>Count (Likelihood)</strong></th>
<th style="text-align: center;"><strong>Posterior Probability</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Blue, Blue, Blue, Blue</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Blue, Blue, Blue, White</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">27 / 38 = 71%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Blue, Blue, White, White</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8 / 38 = 21%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Blue, White, White, White</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3 / 38 = 8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">White, White, White, White</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>The first column in the table shows the five possible bag configurations. The second column contains the number of different ways that the Blue, Blue, White sequence could have occurred. These counts represent the likelihood of each bag configuration—the third key component of Bayesian statistics. After adding the counts in the second column, we see that there are a total of 38 different ways that the sequence could have occurred. The last column turns the counts from the second column into probabilities. Whereas the prior probabilities (20% each) were set before observing the data, the probabilities in the third column were calculated after, and they are therefore aptly named posterior probabilities. Even though we still do not know exactly how many marbles of each color are in the bag, we can use these posterior probabilities to make more informed statements about the contents of the bag. These posterior probabilities become our new prior, replacing the flat prior we used initially. Using this new prior, we could further reduce our uncertainty about the bag’s contents by picking more marbles and following the same procedure to calculate updated posterior probabilities. This process of selecting a prior, collecting data, combining the two through a likelihood, and computing posterior probabilities is Bayesian statistics in a nutshell. We are now in a position to explore in more detail how the abstracted view of this process, which I call the Bayesian framework, relates to critical thinking, and by extension, hyperpartisanship.</p>
</section>
<section id="bayesian-framework" class="level1">
<h1>Bayesian Framework</h1>
<p>When our opinions differ, we often assume that others are either not thinking carefully about the issue or not considering the available information. As a result, one commonly proposed solution is to improve how we teach critical thinking skills so that the next generation is better equipped to evaluate issues. I agree that this task is important, but it suffers from two limitations. First, it only applies to school-aged individuals while neglecting the rest of the population. Second, it leaves “critical thinking” undefined, and we need to agree on what it means to think critically if we want to teach this skill to others. I suggest that Bayesian statistics can help us solve these problems by leading us to a definition that applies to all individuals. In this section, I will outline the connection between Bayesian statistics and a framework for understanding how people form opinions—the Bayesian framework, or simply the framework. This framework both allows us to more accurately reflect on our own and others’ critical thinking skills and illuminates the many factors that can drive differences in opinion, even in a world in which all individuals are thinking critically.</p>
<p>In the previous section, we saw how the three key components of Bayesian statistics—priorrior, likelihood, and data—interact to form a posterior distribution, which can then be used to make informed decisions. If we define critical thinking as the process of analyzing new information in order to make a judgment, then we can already see how it relates to Bayesian statistics. Our preexisting beliefs, which are shaped by past experience and knowledge, are the prior. We update these beliefs in response to new information: our data. The likelihood is how we process this information; it is the ideology through which we combine our existing beliefs and new information. The result is an updated set of beliefs that we use to make decisions until new information arrives and the process repeats.</p>
<section id="prior" class="level2">
<h2 class="anchored" data-anchor-id="prior">Prior</h2>
<p>In Bayesian statistics, a prior is a way of encoding existing knowledge before observing data. The corresponding notion in the framework is quite similar. Our preexisting beliefs inevitably come into play when we assess new pieces of information. Importantly, these beliefs consist of not only our background knowledge but also our past experiences and biases. In our marble-picking example, we had no background knowledge about the bag’s contents and therefore selected a flat prior, which treated all bag configurations as equally likely. The underlying principle was that we wanted to select a prior that was consistent with what we knew about the bag’s contents. This same principle applies to the framework; we want to incorporate what we know about a topic when assessing new information. In Bayesian statistics, the prior is something that we explicitly choose. Similarly, we have some influence over the role that our preexisting beliefs play when assessing new information.</p>
<p>In Bayesian statistics, priors fall along a spectrum from non-informative to informative. A non-informative prior is loosely defined as one that has a minimal impact on posterior inferences (updated beliefs in the framework). As a result, these posterior inferences are largely driven by data and the likelihood. A flat prior is at the furthest end of the non-informative side of the spectrum. In the framework, a flat prior is analogous to having no preexisting beliefs, such that our updated beliefs are formed exclusively by new information in conjunction with our ideologies. This approach may seem reasonable when we form opinions about topics with which we are unfamiliar. However, in Bayesian statistics—as in opinion formation—completely non-informative priors can lead to posterior distributions that are not useful when making decisions. This situation arises for one of two reasons: either there is little data available, or the data that is available is highly variable. As an example of the first case, consider what would happen in the marble example if we were only allowed to select a single marble. We know that it would be blue or white, which would help to rule out either the all-white or all-blue options; other than that, we would have learned little about what the bag contains. The second situation, having highly variable data, is one that most parents have probably encountered. There is a plethora of competing parenting information available, such that reading all the available information is unlikely to result in any clear conclusions. In Bayesian statistics, one way to avoid these pitfalls is to replace the flat, non-informative prior with a more informative one.</p>
<p>A weakly informative prior plays a supporting role in influencing the posterior distribution. In the framework, this type of prior is analogous to allowing our background knowledge to have some influence on our updated beliefs. In the marble scenario, imagine that we are told that bags with equal numbers of blue and white marbles are produced more frequently than other bags. This information is useful, albeit limited. On the one hand, we could ignore it, but then we are no better off than we were before we had this information. A weakly informative prior in this situation is one that assigns, for example, a 4% higher probability to the 2 blue and 2 white marbles bag and therefore a 1% lower probability to the remaining four options. This example exposes an important question that arises in Bayesian statistics: how informative should the prior be? The corresponding question in the framework is: how much should we allow our preexisting beliefs to influence our opinions? Before we try to answer this question directly, we will first consider what happens if we select a prior that is further on the informative end of the spectrum.</p>
<p>In the framework, an informative prior is one that has an outsized impact on how our opinion changes (or does not change) in the presence of new information. The analogue in Bayesian statistics is a prior that assigns a high degree of probability to a narrow range of possible values. Imagine that we assigned a 60% probability to the bag with 2 blue and 2 white marbles and a 10% probability to the remaining four options. Although the data (blue, blue, white) remains the same, the resulting posterior probabilities here will be much closer to our prior probabilities than they were when we used a flat prior. In this situation, we assigned the prior probabilities somewhat arbitrarily based on vague information from the manufacturer. However, this informative prior would be reasonable if the manufacturer had said, “More than half of all bags produced have equal numbers of blue and white marbles,” rather than that these types of bags are produced “more frequently.” Regardless of the phrasing, it would take an incredible amount of marble-picking to diverge from these prior beliefs. Likewise, in the framework, if we have strong preexisting beliefs, it will take an incredible amount of new information to change our minds. There is nothing inherently wrong with allowing our preexisting beliefs to influence our updated beliefs, but it is necessary for us to accurately assess the degree to which these beliefs are informed by our past experiences and biases rather than our prior knowledge.</p>
<p>There can be more than one valid way to choose a prior in both Bayesian statistics and the framework. All priors fall somewhere along the non-informative/informative spectrum and there are many reasonable options. As a general principle, we can say that the less arbitrary the prior, the better. Practically speaking, applying this principle in the framework means taking the time to first understand where along the spectrum our preexisting beliefs fall and what is driving them. Are we so open to new information that we ignore what we already know? Or conversely, do we hold our beliefs so strongly that no amount of new information can change our minds? If we answer “yes” to the second question, then we must dig deeper to understand where these beliefs originated. In going through this introspective exercise, it will likely become clear how difficult it is to consistently avoid both of these extremes. Armed with this self-knowledge, we should feel a stronger sense of empathy for others who are undoubtedly struggling to strike the same balance. If we are committed to fixing the problem of hyperpartisanship, we must all be willing to do the hard work of both practicing self-reflection to understand and correct our own priors and approaching others with empathy.</p>
</section>
<section id="likelihood" class="level2">
<h2 class="anchored" data-anchor-id="likelihood">Likelihood</h2>
<p>In Bayesian statistics, the likelihood is a function that tells us how likely we are to observe some data given a particular set of values for the unknown parameters. In other words, it is the mechanism through which data influences the posterior distribution. In the marble-picking example, the unknown parameter is the bag’s contents and the likelihood is the count of the number of ways the observed data could have occurred if a particular conjecture about the bag’s contents was true. The analogue in the Bayesian framework is an ideology, or belief system, through which we interpret new information. Just as the likelihood is fundamental to Bayesian statistics, so are the ideologies that we all, either implicitly or explicitly, bring to the table. Our ideologies may not be something that we actively ponder, but they nonetheless mediate how we interpret information.<sup>4</sup> Ideologies come in many forms, are not mutually exclusive, and are not necessarily political in nature. For the purpose of this essay, if it ends in “-ism,” it is probably an ideology; liberalism, conservatism, libertarianism, federalism, stoicism, capitalism, and fascism are all ideologies.</p>
<p>Two individuals thinking critically about the same issue may come to wildly different conclusions because of their differing ideologies. This observation is grounded in Bayesian statistics; if the priors and data are identical, but are passed through two different likelihoods, the posterior distributions will be different. Similarly, two individuals with identical preexisting beliefs who are exposed to the same new information will come to different conclusions if they have different ideologies. Ideologies are an integral part of the process of opinion formation, and they are not inherently good or bad. Instead, just as an informative prior is only problematic when it overshadows relevant information, ideology only becomes a problem when it plays an outsized role in how we update our opinions. In Bayesian statistics, the likelihood is the dominant factor when there is a large amount of data. Analogously, our ideologies have a much greater influence on our opinions when we process a lot of information. This observation suggests that in a world where all individuals are incorporating all available information, we can expect our opinions to diverge along the same lines as our underlying ideologies. If we are aware of this potential outcome, we should hesitate to attribute someone’s conflicting view to that person’s failure to accurately assess the available information. Ultimately, it is important for us to be transparent about our ideological differences so that when we inevitably reach different conclusions, we can determine the root cause.</p>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>The role that data plays in Bayesian statistics is equivalent to the role of new information in the framework. Just as information is an integral part of every decision we make, any statistical analysis requires data. If there is no data, then the likelihood becomes unusable, and we are left with only the prior. The prior is sufficient for making inferences, but these inferences would be much better if they were informed by data. While ignoring all data may sound extreme, we run into the same problems when we only incorporate some data. To make things concrete, consider how the posterior probabilities would change in our marble example if we decided to ignore the third draw (blue, blue, rather than blue, blue, white). We would no longer assign zero probability to the conjecture that the bag contains no white marbles. As a result, there is more uncertainty about the bag’s contents because we must consider the possibility that the bag contains all blue marbles. A nearly identical parallel exists in the framework. Without new information, we must rely entirely on our preexisting beliefs to make decisions; if we only incorporate some relevant information, our updated opinions still rely heavily on our preexisting beliefs.</p>
<p>Here we encounter another problem. When multiple individuals choose to incorporate only a subset of the available data, they will often come to different conclusions. In Bayesian statistics, if the priors and likelihoods from two analyses are the same, the posterior distributions are different only if the data is different. Leveraging the marble-picking example again, consider what might happen if a different individual—call him Jack—who has the same background knowledge as Jill also draws three marbles from the bag. If Jack draws three white marbles in a row, he will come to a different conclusion about what the bag likely contains, despite the fact that both he and Jill drew the same number of marbles from the same bag. A particularly stark contrast would develop between their beliefs about whether or not the bag contains all white marbles. Having seen two blue marbles, Jill would vehemently deny the possibility that the bag contains all white marbles, whereas Jack would consider it to be a plausible option. Of course, one obvious solution to this problem would be for Jack and Jill to share their information with each other. We must be similarly open to sharing information when we find ourselves at odds with one another, because even if we are making every effort to critically evaluate the information we access, the mere fact that we access different sources of information can lead us to have divergent viewpoints. Unless we take the time to discuss, we can only make assumptions about the sources of our disagreements and we are left with an incomplete understanding of the situation.</p>
<p>At this point, we have seen how changes to any of the three components of Bayesian statistics (prior, likelihood, and data) result in different posterior distributions. Similarly, when individuals form opinions, differences in preexisting beliefs, ideologies, or information result in divergent beliefs. More importantly, in each instance, there is a reasonable explanation for why two individuals who are thinking critically can come to different conclusions. In order to resolve, or at least understand, these differences, we need to do two things: honestly reflect on what is driving our own opinions and deliberately approach others with empathy in order to understand what is driving their opinions.</p>
</section>
</section>
<section id="a-path-forward" class="level1">
<h1>A Path Forward</h1>
<p>One of my greatest fears in our current political moment is that we will recognize the importance of the issue of hyperpartisanship but neglect to do the work necessary to combat it. I fear that we will instead retreat to the warmth of our social and ideological bubbles. I fear that after the inauguration, there will be widespread complacency among those who supported President-elect Biden’s presidential campaign and despondency, frustration, and anger among those who did not. More importantly, I fear that for the next four years, there will be minimal productive dialogue between individuals from these two groups. Productive dialogues are absolutely necessary if we hope to make any lasting progress towards solving the many problems that we currently face, as well as those that will arise in the future. For most of us, these dialogues will not happen automatically, and therefore we must seek them out. Despite these fears, I also have hope that the 2020 election cycle was enough to shake us out of our stupor and that it will lead us to fully recognize the perilous nature of the current political climate. To those who have placed their hope in the upcoming exchange of power as the event that will “get us on the right track,” I must say that I respectfully disagree. While the president plays a major role in setting the tone for the nation, he alone does not have the capacity to affect the change that is required to solve the problem of hyperpartisanship—the thorn in the side of our democracy.</p>
<p>In order to counteract hyperpartisanship, we must first revisit the conditions that caused it to develop. In the first section, I explained how attempting to incorporate all of the information that is at our fingertips forces us to use time-saving heuristics. These heuristics that mitigate information overload are only problematic when they are inconsistent with thinking critically. We saw that the limited number of political ideologies in the United States, together with ideologically-biased sources of information, prompts us to use heuristics that are not only inconsistent with thinking critically but also lead to political polarization. While we, as individuals, are unable to restructure the two-party system or make news sources ideologically neutral, we do have control over the heuristics we employ in response to information overload. Rather than resorting to time-saving heuristics that cause us to drift towards ideological extremes, we need a new approach that both mitigates information overload and is consistent with the model of critical thinking provided by the Bayesian framework.</p>
<p>With this goal in mind, I propose that each of us choose to focus on one policy area, such as economic, environmental, or education policy. By limiting our focus to a single topic, we put ourselves in a better position to have enough time to survey the available information, evaluate it, and incorporate it into our beliefs. From there, I suggest that we each identify two individuals who hold different ideological beliefs from our own, with whom we will discuss political topics. Friends or family who do not share our beliefs are the perfect place to start. Then, perhaps with the input of these two individuals, we should seek out three new sources of information to help prevent the effects of only accessing information that aligns with our existing beliefs. To promote feelings of empathy towards those with differing political ideologies, I suggest that we compare our views with those of the two dominant political parties; identify two positions held by our political party (or its figurehead) that we find questionable and two positions held by the opposing political party with which we at least partially agree. These four positions do not necessarily have to fall into the single area that we selected initially, although that may be helpful. Finally, since selecting a single policy area may be insufficient to mitigate information overload, I suggest selecting, at most, five specific issues within that area to investigate more deeply. Since every plan needs a name, I am calling this plan “The Countdown”: five specific issues, four positions, three new sources of information, two people, and one policy area.</p>
<p>At a time when many of us pause to reflect on the past year and make resolutions for the next, I hope you will join me in adding “The Countdown” to your list of 2021 resolutions. I suspect that if enough of us take these steps, we will discover both greater common ground and more nuance among our respective ideologies than are captured by the false dichotomy of liberalism versus conservatism. Regardless of whether or not you choose to adopt this resolution, I hope that you will engage others with empathy and, through honest self reflection, have the courage to ask yourself where you fail to think critically.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I have limited my scope in this way based on the premise that each of us, to varying degrees, has the ability to choose how we respond to the incentives and rules promulgated by the institutions in which we participate and the systems that they compose. If we accept this premise, then it follows that many of the properties of our environments are emergent phenomena, therefore we do ourselves a disservice by operating as though “the system” has agency. I will not explore this idea further in this essay, which I expect to be immensely unsatisfying for some. However, I hope that the ideas that I present will be sufficiently thought-provoking that it is worth the time investment to continue reading, even if you reject this premise.↩︎</p></li>
<li id="fn2"><p>There is much more that can and should be said about the role of the media in increasing polarization, but, again, this topic is beyond the scope of this essay. ↩︎</p></li>
<li id="fn3"><p>In this section, I lean heavily on Richard McElreath’s excellent book, Statistical Rethinking, which provides a great introduction to this topic.↩︎</p></li>
<li id="fn4"><p>The irony is that this statement is itself ideologically-grounded and therefore not one that I expect to be shared by every reader. The underlying premise is that complete objectivity is an unattainable ideal, and it is therefore better to aim to be aware of our ideological baggage rather than to eliminate it.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>essay</category>
  <guid>https://www.zduey.github.io/posts/courage-to-ask.html</guid>
  <pubDate>Sun, 03 Jan 2021 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Econometrics for Data Scientists</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/econometrics-for-data-scientists.html</link>
  <description><![CDATA[ 





<p>One of my favorite parts about being in an R&amp;D group is that we have a periodic “Journal Club” where we all read a paper that is relevant to some aspect of projects we are working on, and then discuss. Last year, we were reading some papers about factors that contribute to Parkinson’s Disease (PD) progression. In some cases, the authors were leveraging observational data to make claims (often using causal language) about the relationship between the factors they were studying and the rate of disease PD progression. Given my educational and early professional background in econometrics, I read those papers expecting there to be a focus on establishing and defending an “identification strategy” – the combination of theory and methodology that help justify a causal claim. To my surprise, these papers neither made such an effort nor leveraged the methods that I expected: difference in differences, instrumental variables, regression discontinuity, etc.</p>
<p>In talking about these papers with my co-workers, it was clear that I was in the minority with my surprise at the lack of an identification strategy. This sparked the idea to put together some materials that I tentatively titled “Econometrics for Data Scientists.” Around the same time, I was also finishing up a course in Statistical Learning at Penn (<a href="https://apps.wharton.upenn.edu/syllabi/2020A/STAT974401/">STAT-974</a>). The course emphasized the algorithmic nature of inferential methods and largely de-emphasized causal inference as an enterprise. However, towards the end of the course, we covered a technique called “<a href="https://economics.mit.edu/files/12538">double ML</a>” – a method for leveraging machine learning techniques for doing causal inference. The timing seemed too perfect to pass up: over the next few weeks I put together a series of <a href="https://github.com/zduey/metrics">Jupyter notebooks</a> and <a href="https://docs.google.com/presentation/d/15I1aXxlZCDbVoqGoqPJNl6654gGP_FZrBE6nQMSBCsQ/edit?usp=sharing">slides</a> covering some core econometric methods all the way through to some more recent methods that bridge the machine learning/causal inference divide. For anyone looking to learn more, the slides contain numerous links (organized by topic) to other useful books, articles, and blog posts.</p>



 ]]></description>
  <category>presentations</category>
  <category>caual inference</category>
  <category>econometrics</category>
  <guid>https://www.zduey.github.io/posts/econometrics-for-data-scientists.html</guid>
  <pubDate>Sun, 17 May 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Statistics – Lunch-and-Learn Presentation</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/intro-bayesian-statistics-presentation.html</link>
  <description><![CDATA[ 





<p>A few weeks ago, I completed a graduate-level Bayesian Statistics course at Penn (<a href="https://apps.wharton.upenn.edu/syllabi/2020A/STAT927001/">STAT-927</a>). Although I have been interested in Bayesian Statistics for quite a while now, this was the first formal course I have taken. Professionally, I developed some Bayesian multilevel regression models during my time at the <a href="https://zduey.github.io/miscellaneous/bayesian-statistics-critical-thinking-part1/">Computational Memory Lab</a> as part of the <a href="https://www.darpa.mil/program/restoring-active-memory">RAM project</a>. However, in my current position, I have not typically leveraged fully Bayesian approaches.</p>
<p>Last week at work, I gave an overview of Bayesian Statistics during a lunch-and-learn style session. The intention was to have the material be accessible to anyone with some background in probability and at least a high-level understanding of classical statistics (e.g.&nbsp;at least some familiarity with p-values, hypothesis testing, and confidence intervals). The structure of the presentation largely follows the outline from the first couple of lectures from the course (albeit at a slightly higher level), but with the addition of a running example.</p>
<p>I had a lot of fun putting the slides together and wrestling with how to present the material to a mixed audience (data curators, data scientists, and software engineers). As putting a presentation together often does, it solidified my understanding in many places and pointed to some areas where it is still lacking. I’m hoping to delve into some additional techniques from the course in a future presentation (probably geared towards a smaller audience). In the meantime, here are the <a href="https://docs.google.com/presentation/d/1B-_2ApMT1vVwZ_N17jVLvDwhuG4sbAdiXX9taQh0jEQ/edit?usp=sharing">slides</a> and <a href="https://github.com/zduey/intro-bayes-stat-presentation">code</a> from the initial presentation.</p>



 ]]></description>
  <category>presentations</category>
  <category>bayesian statistics</category>
  <guid>https://www.zduey.github.io/posts/intro-bayesian-statistics-presentation.html</guid>
  <pubDate>Sun, 17 May 2020 00:00:00 GMT</pubDate>
</item>
<item>
  <title>2017 Retrospective and 2018 Goals</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/2017-retrospective-2018-goals.html</link>
  <description><![CDATA[ 





<p>2017 went by in a hurry. My wife and I made the move from Chicago to Philadelphia for her to start a PhD program, which also entailed me switching jobs. After some initial adjustment, work is going well and I have found myself continuing to enjoy writing code for a living. Since switching away from economics research as a career track in 2016, I have been trying to sort out exactly how far towards software engineering I want to veer. My current work is a mixture of model building/validation/testing and more traditional programming projects. Quite honestly, this mixture suits me well, although I often worry about being a jack of all trades and master of none. Although I have no regrets about moving away from the academic research career path, I do wonder whether I’ll need to go back to grad school for people to take my quantitative skills seriously. In the meantime, the plan for 2018 is to continue filling in gaps in my technical background, while adding some more tools to my applied statistics belt.</p>
<section id="topics" class="level3">
<h3 class="anchored" data-anchor-id="topics">Topics</h3>
<ul>
<li>Containers &amp; Docker – I worked through a few tutorials and set up my own container for running a small webserver that runs a service which builds 3D brain visualizations using a few somehwat challenging to install pieces of software. I wish I had taken a deeper dive to understand more about <em>how</em> containers work.</li>
<li>Bayesian Statistics – I had the chance to do some applied work in this area through my current job by building some bayesian hierarchical models that we use in production, so I ended up on a pretty deep dive on this one. I’ll probably put together shorter blog pointing out some of the helpful resources I found/used in the process.</li>
<li><del>Time Series Analysis</del> – Although some of the analysis I do at work has a strong time series component, I did not end up pursuing any side projects like I had originally planned. I’ll call this one a failed goal.</li>
<li>Web programming – Throughout the year, I worked on 4 web-based applications. The first one was a job search app that would take a job title and city, scrape indeed, and store results in a database for later review. I built it with Flask and Mongo DB and used it when I was job hunting in the early part of the year. I ended up adding on to this with by using airflow to automatically search for a set of job titles and cities every hour, which I would then review nightly to see if anything interesting popped out. Interestingly enough, the job I have now was actually one of the results returned during this automated web scraping phase. The second application is an application that I initially built in the first few weeks of my current job that we use for viewing 3D brain visualizations, running ad-hoc queries to our database, and for viewing PDF reports related to the experiments we run. The third project I started working on with my wife since we are both foodies and enjoy cooking together. The idea is to build a social-networking site for food lovers. The initial prototype is designed primarily to make it easier for people to digitize the recipes they commonly use. The last project came out of a civic tech hackathon hosted by the <a href="https://thereentryproject.org/">Reentry Project</a>. I worked with a group of people to prototype a web application that is designed to match mentors with re-entering citizens. One nice thing was that the use case was similar enough to the foodie website that I was able to re-use most of the code. I’ve continued to work with a subset of that initial group to move the prototype closer to production, which will hopefully be finished some time in 2018.</li>
<li>Data Pipelines – Largely because of my current work, I’ve worked with 4 different ‘libraries’ for building data-pipelines: airflow, luigi, joblib, and a homegrown solution. While they all solve similar problems, after working with them to solve real problems, I now have a clearer understanding of the best use cases for each. As I’m thinking about it, there is probably a useful blog post to be written going through this in more detail. I came across a few while I was researching my options initially, but nothing that was as detailed as I would have liked looking back.</li>
</ul>
</section>
<section id="books" class="level3">
<h3 class="anchored" data-anchor-id="books">Books</h3>
<ul>
<li><a href="https://www.amazon.com/Cython-Programmers-Kurt-W-Smith/dp/1491901551/ref=sr_1_1?ie=UTF8&amp;qid=1484107172&amp;sr=8-1&amp;keywords=cython">Cython</a> – I remebmer finding the book helpful at the time, although since I did not do any side projects or work requiring cythonizing python code, I’m sad to say I’ve forgotten most of what was in there at this point.</li>
<li><del><a href="https://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343/ref=sr_1_1?ie=UTF8&amp;qid=1484107234&amp;sr=8-1&amp;keywords=big+data%3A+principles+and+best+practices">Big Data: Principles and best practices of scalable realtime data systems</a></del> – I started reading this, but at the time, I did not have much of a chance to apply it professionally, so I cut the reading short.</li>
<li><a href="https://www.amazon.com/Flask-Web-Development-Developing-Applications/dp/1449372627/ref=sr_1_1?ie=UTF8&amp;qid=1484107257&amp;sr=8-1&amp;keywords=flask+web+development">Flask Web Development</a> – Working through this book was how I built the job search application I described earlier. Looking back, this is the approach I should take when reading any technical book. It really helps to find a toy or side project that you can use to apply what you learn while reading.</li>
<li><a href="https://www.amazon.com/Think-Bayes-Bayesian-Statistics-Python/dp/1449370780/ref=sr_1_1?ie=UTF8&amp;qid=1484107459&amp;sr=8-1&amp;keywords=think+bayes">Think Bayes</a> – This was one of the first books I read in 2017 and sadly I don’t remember a ton about it other than that it was a helpful introduction to the topic of Bayesian statistics, but that I would have liked it to be a bit more technical. However, the book was intended to be for programmers looking to learn Bayesian statistics.</li>
<li><a href="https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?ie=UTF8&amp;qid=1484107411&amp;sr=8-1&amp;keywords=data+analysis+using+regression+and+multilevel+hierarchical+models">Data Analysis Using Regression and Multilevel/Hierarchical Models</a> – This book was constantly by my side as I was building the hierarchical models I mentioned earlier. This is the book I would recommend to someone who has had an introductory statistics course and wants a crash course in applied statistics.</li>
<li><del><a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?ie=UTF8&amp;qid=1484107432&amp;sr=8-1&amp;keywords=bayesian+data+analysis">Bayesian Data Analysis</a></del> (Pushed to 2018)</li>
<li><a href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445/ref=sr_1_1?ie=UTF8&amp;qid=1514759892&amp;sr=8-1&amp;keywords=statistical+rethinking">Statistical Rethinking</a>: By far my favorite technical book of 2017. Although I regret not working through the R-based examples that are sprinkled throughout the text, I found the explanations of concepts incredibly clear and consise. After reading it, I feel like I now have the intuition and level of understanding to really dive in to the topic of bayesian data analysis.</li>
<li><a href="https://www.amazon.com/Two-Scoops-Django-1-11-Practices/dp/0692915729/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1514759910&amp;sr=1-1&amp;keywords=two+scoops+of+django">Two Scoops of Django</a>: Extremely helpful in learning how to effectively and idiomatically build Django-based web applications.</li>
<li><a href="https://www.amazon.com/Analyzing-Neural-Time-Data-Neuropsychology/dp/0262019876/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1514759926&amp;sr=1-1&amp;keywords=analyzing+neural+time+series+data">Analyzing Neural Time Series Data</a> – I read this as part of getting up to speed for my current work. One of the highlights for me was an intuitive explanation of Fourier transforms.</li>
<li><a href="https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1514759942&amp;sr=1-1&amp;keywords=the+mythical+man+month">The Mythical Man-Month</a> – A classic for those who are interested in how software gets made. I was pleasantly surprised to find that much of the advice/observations apply as much to building software in the 21st century as they did when the book was originally published in 1975.</li>
</ul>
</section>
<section id="projects" class="level3">
<h3 class="anchored" data-anchor-id="projects">Projects</h3>
<ul>
<li>Set up home workstation as a server</li>
<li>Contribute to an open source project</li>
<li>Build a docker container image</li>
<li><del>Compete in Kaggle’s Two Sigma code competition</del></li>
<li>Build a simple analytics web app</li>
</ul>
</section>
<section id="coursework" class="level3">
<h3 class="anchored" data-anchor-id="coursework">Coursework</h3>
<ul>
<li><a href="https://www.safaribooksonline.com/library/view/learning-path-scaling/9781491977804/">Scaling Python for Big Data</a></li>
<li><a href="https://www.udacity.com/course/deep-learning--ud730">Deep Learning</a></li>
</ul>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">2018</h2>
<section id="topics-1" class="level3">
<h3 class="anchored" data-anchor-id="topics-1">Topics</h3>
<ul>
<li>Operating Systems</li>
<li>Networking</li>
<li>C++</li>
<li>Web Programming</li>
<li>Bayesian Statistics</li>
</ul>
</section>
<section id="books-1" class="level3">
<h3 class="anchored" data-anchor-id="books-1">Books</h3>
<ul>
<li><a href="https://www.amazon.com/Modern-Operating-Systems-Andrew-Tanenbaum/dp/013359162X/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1514758673&amp;sr=1-1&amp;keywords=Operating+Systems+Tanenbaum">Modern Operating Systems</a></li>
<li><a href="https://www.amazon.com/gp/product/020170353X/ref=ox_sc_act_title_1?smid=ATVPDKIKX0DER&amp;psc=1">Accelerated C++</a></li>
<li><a href="https://www.amazon.com/Effective-Modern-Specific-Ways-Improve/dp/1491903996/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1514758711&amp;sr=1-1&amp;keywords=effective+modern+c%2B%2B">Effective Modern C++</a></li>
<li><a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?ie=UTF8&amp;qid=1484107432&amp;sr=8-1&amp;keywords=bayesian+data+analysis">Bayesian Data Analysis</a></li>
</ul>
</section>
<section id="projects-1" class="level3">
<h3 class="anchored" data-anchor-id="projects-1">Projects</h3>
<ul>
<li><a href="https://mentorphilly.herokuapp.com/">MentorPhilly Web App</a></li>
<li><a href="http://www.myrecipestash.com/">MyRecipeStash Web App</a></li>
<li>Homework assignments for Software Systems course</li>
<li>Submit PR revising logging in luigi</li>
</ul>
</section>
<section id="coursework-1" class="level3">
<h3 class="anchored" data-anchor-id="coursework-1">Coursework</h3>
<ul>
<li><a href="http://www.cis.upenn.edu/~cis505/">Software Systems</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>reflection</category>
  <guid>https://www.zduey.github.io/posts/2017-retrospective-2018-goals.html</guid>
  <pubDate>Mon, 01 Jan 2018 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Setting Up SSH On A Home Computer</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/ssh-server-on-home-computer.html</link>
  <description><![CDATA[ 





<p>tl;dr – The guide I wish I had when setting up an SSH server on a home pc</p>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>Around 18 months ago, I built a desktop computer so I would have a little more firepower at my fingertips when I needed it. My hope was to avoid having to pay for cloud computing services during Kaggle competitions and other side projects. Early on, I realized that it would be nice to be able to use this machine remotely, so I found a few resources for setting up your home PC as an SSH server. However, I was not able to find a single resource that provided enough background information that I was not just copy/pasting the commands.</p>
<p>Although no advanced knowledge of any particular topic is required to set up your own SSH server, there are many concepts to get your head around. I will explain more detail below, but the short version is that an SSH server is a process running on your computer that waits for outside computers to request access via a specific port, authenticates that user, and then allows access to the computer. If half of those terms do not make sense, do not worry. I was in the same position when I first tried getting set up. Here is a quick list of the concepts/terms that we will cover:</p>
<ol type="1">
<li>Server</li>
<li>SSH</li>
<li>Network Address Translation</li>
<li>Public/Private IP Addresses</li>
<li>Ports</li>
<li>Port Forwarding</li>
<li>Public/Private Key Authentication</li>
</ol>
<p>I use Ubuntu 14.04 LTS as my operating system, however, the steps will be similar on any unix-like platform. To follow along, be sure to have access to the following:</p>
<ol type="1">
<li>The unix-based computer that will host the SSH server</li>
<li>A different computer to test the remote connection to the server</li>
<li>Access to a different network than the one to which your host machine is connected (optional, but recommended)</li>
</ol>
<p>For the purposes of this guide, ‘host’ will indicate the computer running the SSH server, while ‘client’ will refer to any computer requesting access to the host. At one point in this guide, your host computer will also be a client.</p>
</section>
<section id="initial-setup" class="level2">
<h2 class="anchored" data-anchor-id="initial-setup">Initial Setup</h2>
<p>SSH stands for “secure shell” and is what will allow us to establish a secure connection between two computers. Our end goal is to be able to issue commands from a client machine that are executed by the host machine. For a more thorough coverage of SSH, take a look at <a href="https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys">this great guide by Digital Ocean</a>.</p>
<p>The term “server” is often used a bit loosely. Here, we mean by the term is a process running on a computer that is tasked with managing access to a computer’s resources over a network. We will be installing the openssh-server application, which will allow us to run an SSH server on our machine that will handle requests for access to the host computer from other devices.</p>
<p>Start by updating your system packages:</p>
<pre><code>sudo apt-get upgrade</code></pre>
<p>Install the openssh-server application and client. You should also install the openssh-client on machines that will be used as clients.</p>
<pre><code>sudo apt-get install openssh-client
sudo apt-get install openssh-server</code></pre>
<p>You should now have an SSH server process running on your machine. Check with the following:</p>
<pre><code>ps -A | grep sshd</code></pre>
<p>You should see something like:</p>
<pre><code>[number] ?  00:00:00 sshd</code></pre>
<p>For those of you who copy/pasted the above command (something I have certainly been guilty of), you will learn a lot more by taking the time to look up any commands you do not recognize. If you are new (or just want a refresher on) to shell commands, check out the <a href="https://www.codecademy.com/learn/learn-the-command-line">Learn the Command Line</a> course from Code Academy.</p>
<p>Check that you can ‘login’ to the host machine from the host machine, i.e.&nbsp;your host machine is also the client in this case.</p>
<pre><code>ssh localhost</code></pre>
<p>Move around a little in the shell (cd, pwd, etc.) to make sure that everything works and then type “exit” to end the session.</p>
<p>Next, we will connect using a true ‘client’ machine. For this to work, make sure that your client machine is on the same network as the server. If you are following along from your home or work wifi network, you should be all set.</p>
<p>Start by determining the IP address of your host machine by listing information about the network interfaces on your machine.</p>
<pre><code>ifconfig</code></pre>
<p>You should see entries for ‘eth0’ and ‘lo’, as well as some other entries if you have installed a wireless card on your desktop. If your host machine is connected to the network via an ethernet cable, use the address associated with the “eth[number]” entry. If you are connected via wifi, use the “wlan[number]” entry. If you want to see just the list of IP addresses for the devices, you can use:</p>
<pre><code>ifconfig | grep "inet addr"</code></pre>
<p>Next, try logging in from the client machine using that IP address:</p>
<pre><code>ssh username@X.X.X.X</code></pre>
<p>You should be prompted for the password of the account you used in your ssh command. For security reasons, the characters will not be displayed in the shell. If successful, you should see a “Welcome to Ubuntu X.X.X” message or something similar depending on your OS.</p>
<p>This is a bit more useful than our previous test, but it still does not solve our initial problem of being able to access the machine from outside the same network. Go ahead and try it out if you have the ability to connect your client machine to a different network than your host machine.</p>
<p>A quick option if you have a little data to spare on your cellular plan is to turn on tethering and connect your client machine to that network, while your host machine remains on the other network. Once you are on this separate network, try that ssh command again:</p>
<pre><code>ssh username@X.X.X.X</code></pre>
<p>Most likely, you will notice that the command hangs and then eventually times out. Go ahead and disconnect from this other network and we will finish with the last step of the setup. But first, we will take a second to cover a few more background concepts that will make this next step clearer.</p>
</section>
<section id="nat-and-publicprivate-ip-addresses" class="level2">
<h2 class="anchored" data-anchor-id="nat-and-publicprivate-ip-addresses">NAT and Public/Private IP Addresses</h2>
<p>It may be surprising, but the internet is only able to handle a finite number of users being on line at a single time. In a world where almost every electronic device has access to the internet, it is possible to get close to the theoretical maximum. In order to circumvent this problem, Network Address Translation (NAT) was created to allow a set of devices on a private network to share a single IP address. On the private network, each device receives its own private IP address. For example, the IP address you found above with “ifconfig” is the private IP address of your computer on your local network.</p>
<p>Any time you acess a web page from one of the devices on your home network, the request is routed (hence the term router) through an NAT device, which translates the private IP address into a request using the public IP address assigned to the router by your ISP. This is the IP address you will ultimately need to use when connecting from outside your local network. Go ahead and determine your public IP address by going to www.whatsmyip.org. You should see something in the form of XX.XX.XXX.XXX. You may be tempted to try that ssh command again with this address, but in all likelihood it still will not work.</p>
<p>Although NAT solves the device-limit problem, it adds a layer of complexity to setting up a home computer to accept SSH connections. When a client machine sends a request to connect to the public IP address, your router does not know which of the devices on your private network the request is meant for. Luckily, the solution is usually straightforward and involves setting up ‘port fowarding’ on your router. Bear with me as we continue down the terminology rabbit hole and go over ports. Feel free to skip ahead if you are already familiar with the concept.</p>
</section>
<section id="ports-and-port-fowarding" class="level2">
<h2 class="anchored" data-anchor-id="ports-and-port-fowarding">Ports and Port Fowarding</h2>
<p>Although computers have a variety of physical ports that most people are familiar with (USB, HDMI, VGA, etc.), the ports I am referring to here are networking ports and are logical, not physical. When you start a server (i.e. the SSH server you just set up in the previous steps), you bind it to one of these logical ports. The process then ‘listens’ for messages sent to this port by other programs (either internal or external). Each port has a number and some of these numbers are reserved for use by specific types of services. As an example, servers handling HTTP request use port 80 as the default. When you access a web page via a URL, you are really sending a message to an HTTP server at port 80 for a specific resource (web page or other content) that is managed by that service. In general, port 22 is used for the SSH service.</p>
<p>In this context, port forwarding means telling your router to forward requests made using a specific port to a particular device on your private network. It is then the responsibility of that device to handle the request. You may be wondering: why don’t just send the request directly to the computer via the private IP address and avoid all of this port forwarding nonsense? Like the name implies, these IP addresses are private, so once you are on a different network, the internet-at-large has no knowledge of this private IP address and therefore the request to connect will fail.</p>
<p>Every router is a little bit different when it comes to setting up port forwarding. This may sound like a cop out, but honestly, your best bet is to look up directions for your specific router. For example, here are instructions for an <a href="https://www.att.com/esupport/article.html#!/u-verse-high-speed-internet/KM1010280">AT&amp;T router</a> and <a href="https://www.xfinity.com/support/internet/port-forwarding-xfinity-wireless-gateway/">Comcast XFINITY</a>. In general, you will need to take the following actions:</p>
<ol type="1">
<li>Log in to your router’s admin page</li>
<li>Navgiate to the page for adding a service (SSH is usually one of the default options)</li>
<li>Select or enter the port number where requests will be made (22 by default for SSH)</li>
<li>Select or input the private IP address you found earlier of your host machine</li>
<li>Save the updated settings</li>
</ol>
<p>Now, we can repeat our ssh command using the public IP address and that request should be redirected by our router to our host machine. Connect back to that outside network and try:</p>
<pre><code>ssh username@[public IP address]</code></pre>
<p>You should again be prompted for your host machine password and then admitted access.</p>
</section>
<section id="more-secure-ssh-server-configuration" class="level2">
<h2 class="anchored" data-anchor-id="more-secure-ssh-server-configuration">More Secure SSH Server Configuration</h2>
<p>For this final section, we will be making some changes to the configuration settings for our SSH server. On Ubuntu, these settings are located in the /etc/ssh/sshd_config file. A couple of quick notes:</p>
<ul>
<li>To edit this file, you will need to open it as a super user</li>
<li>Any time you update the config settings, you need to restart the SSH server to have them take effect</li>
</ul>
<pre><code>sudo restart ssh</code></pre>
<p>When a running SSH server receives a request over port 22 (or whatever port it is bound to), it must check that the requesting user has permission to access the computer. This can be done with a password, however, it is considered much more secure to use public key authentication. Public key cryptography works by having a user generate a public key that can be given to anyone, while keeping the private key hidden. Authentication works by using the public key (made available by the user to the process doing the authentication) to verify that the requesting user (whose request to access the service is encrypted using their private key) matches a user who is allowed access. Once this connection is established, more efficient methods of data encryption/decryption are used to transfer the keystrokes and other information using this secure connection.</p>
<p>If you have not already done so, you will need to generate a public/private key pair to use when logging in to your machine. Note that you will need to do this on any device you wish to be allowed access to your SSH server. There are a number of good guides out there for doing this, so to avoid re-inventing the wheel, I suggest following the <a href="https://help.ubuntu.com/community/SSH/OpenSSH/Keys#keys-with-specific-commands">Ubuntu Instructions</a></p>
<p>Now that you have those keys ready, we will enable public key authentication on the SSH server. Open the sshd_config file as a super user for the next set of steps and start by changing:</p>
<pre><code>PasswordAuthentication yes</code></pre>
<p>to</p>
<pre><code>PasswordAuthentication no</code></pre>
<p>This will prevent users from being able to access the server with a password, which will help against a brute force attack that attempts to guess the password. Another recommendation is to change the port that the SSH service uses. Within your config file, comment out the existing port specification and choose a new port.</p>
<pre><code># Port 22
Port [new port number]</code></pre>
<p>Remember, based on an earlier step, your router will still attempt to send SSH requests to port 22, so you will need to go back into your router’s config settings and update the port number for the SSH service. If you are on a machine with multiple user accounts, you can also limit which users are allowed to log in through SSH. At the bottom of the config file add:</p>
<pre><code>AllowUsers [user1] [user 2] ...</code></pre>
<p>You can also deny specific users and add/deny groups, however, it is unlikely that you will need to do this for a home computer.</p>
<p>Next, disable root login via SSH by changing:</p>
<pre><code>PermitRootLogin without-password</code></pre>
<p>to</p>
<pre><code>PermitRootLogin No</code></pre>
<p>Beyond disabling password logins, you can also prevent brute force attacks by limiting the number of concurrent connections to the SSH server. For a home computer, keep this number low by changing:</p>
<pre><code>#MaxStartups 10:30:60</code></pre>
<p>to</p>
<pre><code>MaxStartups 3</code></pre>
<p>With an SSH connection, you can tunnel graphical windows. If you do not plan to use this ability,then go ahead and shut it off by changing</p>
<pre><code>X11Forwarding yes</code></pre>
<p>to</p>
<pre><code>X11Forwarding no</code></pre>
<p>In addition to forwarding windows, you can also forward ports via SSH. For example, if you had a web server running on port 80 on your host machine, you could forward that port to a port on your client machine. This can be handy, but if you do not plan on using this ability, then disable it until you have a need for it since it can be used maliciously.</p>
<pre><code>AllowTcpForwarding no</code></pre>
<p>There are a bunch of other settings in the ssh_config file, but I do not recommend changing the defaults unless you have a good reason to do so. While the suggested changes will add some security to your server, there are many additional ways to add security. In fact, they probably deserve a post of their own, but I will save that for another time.</p>
<p>Thanks for taking the time to read through this and please feel free to let me know if anything is unclear.</p>
</section>
<section id="external-resources" class="level2">
<h2 class="anchored" data-anchor-id="external-resources">External Resources</h2>
<ul>
<li><a href="https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys">Digital Ocean SSH Essentials</a></li>
<li><a href="https://help.ubuntu.com/community/SSH/OpenSSH/Configuring">Ubuntu SSH Configuration Guide</a></li>
<li><a href="https://help.ubuntu.com/community/SSH/OpenSSH/Keys#keys-with-specific-commands">Ubuntu Public/Private Keys Guide</a></li>
<li><a href="https://www.maketecheasier.com/secure-ssh-server-ubuntu/">Secure SSH Configuration</a></li>
</ul>


</section>

 ]]></description>
  <category>tutorial</category>
  <guid>https://www.zduey.github.io/posts/ssh-server-on-home-computer.html</guid>
  <pubDate>Sat, 26 Aug 2017 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Deploying an Application with Heroku</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/app-deployment-with-heroku.html</link>
  <description><![CDATA[ 





<p>This is a follow-up to an <a href="https://zduey.github.io/snippets/streaming-stock-data-with-bokeh/">earlier blog post</a> on building a single-page web app in bokeh. In that post, I went through some of the background on how to build a chart in bokeh that streams data.</p>
<p>This post is about deploying an application using as PaaS provider. It is not a thorough comparison of the various PaaS providers out there, because quite honestly, my objective function was this simple:</p>
<ul>
<li>Do I know about it (binary)</li>
<li>Can I deploy for free (binary)</li>
<li>Ease of use (hand-wavy continuous variable)</li>
</ul>
<p>The providers I was choosing between included:</p>
<ol type="1">
<li>Amazon AWS</li>
<li>Google Cloud Platform</li>
<li>Digital Ocean</li>
<li>Heroku</li>
</ol>
<p>In terms of cost, all of the providers (except Digital Ocean) have some sort of free tier. AWS and Google Cloud Platform are both feature-rich, which is both a blessing and a curse. On the plus side, if my intent was to build something scalable and production-ready, those would likely be the clear front-runners. On the negative side, because of all the features, it takes a bit of effort to navigate all of the microservices and to determine what to include as part of the stack. As a hobbyist (at least in this use case), I needed a very small compute instance with a simple deployment mechanism. In the end, Heroku was the top choice because of its free tier and relatively straightforward (and automated) deployment mechanism. The downside to Heroku’s free tier is that after 30 minutes of inactivity, the application will sleep, which causes the page to load a bit slowly the next time it is accessed. Heroku has very readable and helpful documentation, so if I do a poor job with this overview, go directly to the source where they have plenty of information to get you up and running.</p>
<p>Once you create your Heroku account, I recommend reading <a href="https://devcenter.heroku.com/articles/how-heroku-works">How Heroku Works</a>. It covers all of the concepts you need to get a basic app up and running but without any messy or overly-technical architectural details. After that, work your way through <a href="https://devcenter.heroku.com/articles/getting-started-with-python#introduction">Getting Started on Heroku with Python</a>. As part of the tutorial, you will need to install the Heroku CLI. On my Ubuntu workstation, it was as simple as adding their apt repository. Heroku provides instructions for other operating systems as well.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> add-apt-repository <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"deb https://cli-assets.heroku.com/branches/stable/apt ./"</span> </span>
<span id="cb1-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">curl</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-L</span> https://cli-assets.heroku.com/apt/release.key <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">|</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-key add <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-</span> </span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get update</span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sudo</span> apt-get install heroku </span></code></pre></div>
<p>While you are logged in, go to your dashboard and create a new app. All you need to do is give it a name. The name is not strictly necessary as heroku will randomly choose one if you leave it blank. I named my app <code>iex-streaming</code>. Once you create the app, you will be brought to the deployment screen where you can choose between Heroku Git, GitHub, and Dropbox. I chose to deploy from a GitHub repo. If you go this route, create a repo from within GitHub so that you can connect it to Heroku. Alternatively, you can use Heroku as a remote and push to Heroku directly using the CLI tool. You can also use Dropbox, but I will not talk about that here since I do not see why you would choose that option unless you wanted to avoid having to learn Git. If you are in that boat, then my suggestion would be to rethink that decision and take the time to at least learn the basics of Git. A great way to get started is to go through the <a href="https://try.github.io/levels/1/challenges/1">Try Git</a> tutorial. This tutorial gives you enough to follow along the rest of the way.</p>
<p>With the preliminaries out of the way, there are only a few things that need to be done to get our single page bokeh app deployed. First, turn the folder where your code is stored in a Git repository. You will also need to set up your local repository to track the remote repository. If you are using Heroku Git, the command is a bit different, but you can find the instructions in the deployment page I mentioned earlier.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> init</span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> remote add origin https://github.com/zduey/iex.git</span></code></pre></div>
<p>If you have not already done so, create an environment for the project (I am using conda) and install pandas and bokeh. Once you are done with that, make sure you activate the environment and check that the app still runs locally in case there are any missing dependencies.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-n</span> iex python=3.6</span>
<span id="cb3-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">source</span> activate iex</span>
<span id="cb3-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install bokeh</span>
<span id="cb3-4"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install pandas</span></code></pre></div>
<p>Now, use pip to export the information about your environment to a requirements.txt file. Heroku will use this file to set up your remote machine when you deploy.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> freeze <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> requirements.txt</span></code></pre></div>
<p>Next, create a runtime.txt file. In this file, you will identify the version of python to use for the application. At the time this was written Heroku only supports two python runtimes: python-2.7.13 and python-3.6.0.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">touch</span> runtime.txt</span></code></pre></div>
<pre class="text"><code># runtime.txt
python-3.6.0</code></pre>
<p>If you would like, you can add a README file.</p>
<pre><code>touch README.md</code></pre>
<p>Finally, you need to create a Procfile. This file contains the instructions for launching your application from Heroku. In the simplest case, it can be a single line long.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">touch</span> Procfile</span></code></pre></div>
<pre><code># Procfile
web: bokeh serve --port $PORT --host iex-streaming.herokuapp.com --address=0.0.0.0 --use-xheaders iex.py</code></pre>
<p>This step was the only part of the deployment where I hit a snag. I ended up relying heavily on this <a href="http://stackoverflow.com/questions/38417200/">stackoverflow question</a> to work out the last issues I was having.</p>
<p>Starting on the left, <code>web:</code> is a directive to Heroku indicating the type of process that the command is controlling. In this case, the app requires a single web process. Everything afterwards is the shell command that gets executed to start the app. If you read the earlier post, <code>bokeh serve</code> should be familiar to you. Bokeh comes with a CLI and <code>serve</code> is the command that fires up the bokeh server. Everything else is an argument to this command. <code>--port $PORT --host iex-streaming.herokuapp.com</code> instructs bokeh to listen for requests sent to iex-streaming.herokuapp.com on the port defined by the environment variable <code>$PORT</code>, which is controlled by Heroku. <code>--address=0.0.0.0</code> tells bokeh to listen on all network interfaces for requests. <code>--use-xheaders</code> is a flag indicating that bokeh should use X-headers for IP/protocol information (according to the man page). Networking is not a strong suit of mine (yet… I’ll get there one day), so unfortunately, that last part is still a black box for me.</p>
<p>With all of those files in place, we are just about finished. Your repo should contain the following files:</p>
<ol type="1">
<li>iex.py</li>
<li>Procfile</li>
<li>requirements.txt</li>
<li>runtime.txt</li>
<li>README.md</li>
</ol>
<p>Go ahead and add/commit/push. If you are using Heroku Git, your push will be slightly different.</p>
<pre><code>git add .
git commit -m "Starter files for app"
git push -u origin master</code></pre>
<p>As soon as you push your changes, Heroku will re-deploy your application with the code changes. You can see the build results in your Heroku dashboard and after a minute or two, your changes will be live. Go to your app url and check it out! My sample version is available at:</p>
<p><a href="http://iex-streaming.herokuapp.com/">http://iex-streaming.herokuapp.com/</a></p>



 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <guid>https://www.zduey.github.io/posts/app-deployment-with-heroku.html</guid>
  <pubDate>Sat, 11 Mar 2017 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Streaming Stock Price Data with Bokeh</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/streaming-stock-data-with-bokeh.html</link>
  <description><![CDATA[ 





<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>As part of my 2017 goal to work on a small analytics-oriented web app, I started doing some research into what I would want to use for the visualization component. Being a huge fan of python, I wanted to try out <a href="http://bokeh.pydata.org/en/latest/">bokeh</a>, which touts interactive visualizations using pure python. Bokeh also allows for a number of different demployment options, including within a Flask app, so it seemed like a reasonable option to consider.</p>
<p>For a quick weekend hack, I opted to build a real-time price chart. The <a href="https://www.iextrading.com/">Investors Exchange</a> (IEX) recently released an API that allows you to get the last trade price in real-time (and a bunch of other data for that matter) for equities trading on their exchange.</p>
<p>The scope of the project was very small: build a single page bokeh app that would stream stock price quotes and allow the user to change which ticker to stream. The script boils down to three components:</p>
<ol type="1">
<li>A function to get the last traded price</li>
<li>A callback function to update the ticker being streamed</li>
<li>The code to set up the chart</li>
</ol>
</section>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>The IEX api has a number of different endpoints. The base url is <code>https://api.iextrading.com/1.0</code> and the endpoint for accessing the last traded price is /tops. As an example, if you want the last traded price for Snap, Inc.&nbsp;you would send a GET request to: <code>https://api.iextrading.com/1.0/tops?symbols=SNAP</code>. IEX provides very clear <a href="https://www.iextrading.com/developer/">documentation</a>, so I won’t go into more detail about usage.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> io</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> requests</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-4"></span>
<span id="cb1-5"></span>
<span id="cb1-6">base <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://api.iextrading.com/1.0/"</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_last_price(symbol):</span>
<span id="cb1-9">    payload <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb1-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"format"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"csv"</span>,</span>
<span id="cb1-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"symbols"</span>: symbol</span>
<span id="cb1-12">    }</span>
<span id="cb1-13">    endpoint <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tops/last"</span></span>
<span id="cb1-14"></span>
<span id="cb1-15">    raw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> requests.get(base <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> endpoint, params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>payload)</span>
<span id="cb1-16">    raw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> io.BytesIO(raw.content)</span>
<span id="cb1-17">    prices_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(raw, sep<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">","</span>)</span>
<span id="cb1-18">    prices_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.to_datetime(prices_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time"</span>], unit<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ms"</span>)</span>
<span id="cb1-19">    prices_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"display_time"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prices_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time"</span>].dt.strftime(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"%m-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">-%Y %H:%M:%S.</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%f</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-20"></span>
<span id="cb1-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> prices_df</span></code></pre></div>
<p>There are a few things to note about this section. First, I wanted to get the data into a pandas dataframe to do a little post-processing. Using the io library, you can store data in a memory buffer and then read out of that buffer with pandas. This saves you overhead of doing disk i/o. Second, the IEX API returns all times in milliseconds since the Unix epoch. Creating the <code>display_time</code> variable is done in order to have a nicely-formatted date in the tooltip for the chart.</p>
<p>Bokeh has nice integration with pandas.The best option when your data is in a pandas dataframe is to use a <code>ColumnDataSource</code> object, which takes either a dictionary or a pandas dataframe as an argument. I had some trouble with constructing the <code>ColumnDataSource</code> directly from the pandas dataframe because it will include the dataframe’s index as a column. Instead, I went with a slighly clunkier option and explicitly created a dictionary.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ColumnDataSource(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], display_time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], price<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[]))</span></code></pre></div>
<p>Once the data source is set up, there are a number of methods availabe. However, the only one I will be using is <code>stream()</code> which allows you to append new data to existing columns in your data source.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> update_price():</span>
<span id="cb3-2">    new_price <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_last_price(symbol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>TICKER)</span>
<span id="cb3-3">    data.stream(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>new_price[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time"</span>],</span>
<span id="cb3-4">                     display_time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>new_price[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"display_time"</span>],</span>
<span id="cb3-5">                     price<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>new_price[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"price"</span>]), </span>
<span id="cb3-6">                <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>)</span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span></span></code></pre></div>
<p>One thing to note is the second argument to <code>stream()</code>, which controls how many datapoints will be kept in the datasource before rolling off. If you have a large datasource, keeping this parameter reasonably small will keep your browser from getting bogged down in rendering the chart.</p>
</section>
<section id="updating-the-ticker" class="level2">
<h2 class="anchored" data-anchor-id="updating-the-ticker">Updating the Ticker</h2>
<p>Your <code>ColumnDataSource</code> object has a <code>data</code> attribute, which is the dictionary containing the underlying data. Directly modifying this attribute is what allows me to update what ticker is being streamed. When a user submits a new ticker to stream, this attribute is reset and begins receiving data after the next request to the IEX API.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">TICKER <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> update_ticker():</span>
<span id="cb4-4">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">global</span> TICKER</span>
<span id="cb4-5">    TICKER <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ticker_textbox.value</span>
<span id="cb4-6">    price_plot.title.text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IEX Real-Time Price: "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ticker_textbox.value</span>
<span id="cb4-7">    data.data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], display_time<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[], price<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[])</span>
<span id="cb4-8"></span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span></span></code></pre></div>
<p>One thing I am not a fan of is the apparent need to make TICKER a global variable. So far as I can tell, you cannot pass args to your callback functions. It is possible that I am mistaken about this, so I will post an update if I find a better way. The <code>ticker_textbox</code> is the bokeh text input widget. When <code>update_ticker()</code> is called, it uses the current value of that input widget as the new ticker to stream. The funciton also updates the title in the bokeh figure to reflect the currently-streaming ticker.</p>
</section>
<section id="setting-up-the-dashboard" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-the-dashboard">Setting up the Dashboard</h2>
<p>The dashboard is very simple, but there are still a number of components involved. First, to create the tooltip, you do so by creating a HoverTool object.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">hover <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> HoverTool(tooltips<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[</span>
<span id="cb5-2">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Time"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"@display_time"</span>),</span>
<span id="cb5-3">    (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IEX Real-Time Price"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"@price"</span>)</span>
<span id="cb5-4">    ])</span></code></pre></div>
<p>The <code>@</code> syntax corresponds to variables in your data source. Any column in the data can be added to your tooltip. In this case, I am displaying the price and my formatted time variable.</p>
<p>To set up the plot, you create a <code>figure</code> object. Many of the figure’s attributes can be configured during creation, or you can access and modify them after the fact. You can see both below:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">price_plot <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> figure(plot_width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>,</span>
<span id="cb6-2">                    plot_height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>,</span>
<span id="cb6-3">                    x_axis_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datetime'</span>,</span>
<span id="cb6-4">                    tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[hover, ResizeTool(), SaveTool()])</span>
<span id="cb6-5"></span>
<span id="cb6-6">price_plot.line(source<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'time'</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'price'</span>)</span>
<span id="cb6-7">price_plot.xaxis.axis_label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Time"</span></span>
<span id="cb6-8">price_plot.yaxis.axis_label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IEX Real-Time Price"</span></span>
<span id="cb6-9">price_plot.title.text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"IEX Real Time Price: "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> TICKER</span></code></pre></div>
<p>Next, I needed to create two widgets: a textbox for capturing tickers and a button to trigger the update. You can set a callback function for a widget by passing a callable to the <code>on_click()</code>method of a widget. Finally, I bound the two user input widgets together in a widgetbox, which provides the benefit of ensuring all widgets have the same sizing mode.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">ticker_textbox <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TextInput(placeholder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Ticker"</span>)</span>
<span id="cb7-2">update <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Button(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Update"</span>)</span>
<span id="cb7-3">update.on_click(update_ticker)</span>
<span id="cb7-4"></span>
<span id="cb7-5">inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> widgetbox([ticker_textbox, update], width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>)</span></code></pre></div>
<p>The last piece is to finish setting up the current document. I arrange the widgetbox and the figure into a single row and bind that to the view. Finally, to stream the data I specify <code>update_price()</code> as the callback function to use at a fixed interval of 1 second (1000ms).</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">curdoc().add_root(row(inputs, price_plot, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1600</span>))</span>
<span id="cb8-2">curdoc().add_periodic_callback(update_price, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span></code></pre></div>
</section>
<section id="running-the-app" class="level2">
<h2 class="anchored" data-anchor-id="running-the-app">Running the App</h2>
<p>The full code is available as a <a href="https://gist.github.com/zduey/66ed98cf3fc2b161df47c0c08954dc62">gist</a>. Download/clone/etc. the script and then run <code>bokeh serve iex.py</code> from the command line. The bokeh server will fire up and display the dashboard at port 5006. Type in your ticker, hit update, and the price data will begin streaming. Keep in mind that you will only get streaming data <a href="https://www.iextrading.com/trading/">when the market is open</a>. If you choose a lightly-traded product, it will be less interesting, so I recommend starting with a big-name firm. For a complete list, IEX provides a regularly-updated <a href="https://www.iextrading.com/trading/eligible-symbols/">list of tickers</a>.</p>


</section>

 ]]></description>
  <category>tutorial</category>
  <category>python</category>
  <guid>https://www.zduey.github.io/posts/streaming-stock-data-with-bokeh.html</guid>
  <pubDate>Sun, 05 Mar 2017 00:00:00 GMT</pubDate>
</item>
<item>
  <title>2016 Retrospective and 2017 Goals</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/2016-retrospective-2017-goals.html</link>
  <description><![CDATA[ 





<p>2016 was a pretty good year. I transitioned away from an earlier stage of work life where I thought economics research was all I wanted to be doing. As it turns out, seeing how the research sausage gets made can be just as ugly as seeing how any other sausage gets made. After 18 months as a research assistant, I took a new job with more of a software development bent. For 2016 and again for 2017, I’ve tried picking out a few areas that I want to focus on in my reading, side projects, etc. Otherwise, I end up with way too much breadth and not enough depth. I’m probably still too far on the breadth side, but hey, there is always 2018…</p>
<section id="in-retrospect" class="level1">
<h1>2016 in Retrospect</h1>
<section id="topics" class="level2">
<h2 class="anchored" data-anchor-id="topics">Topics</h2>
<ul>
<li>Coding best practices</li>
<li>Statistical/Machine Learning</li>
<li>GUI Programming</li>
</ul>
</section>
<section id="books" class="level2">
<h2 class="anchored" data-anchor-id="books">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=sr_1_1?ie=UTF8&amp;qid=1484107927&amp;sr=8-1&amp;keywords=code+complete">Code Complete</a></li>
<li><a href="https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X/ref=sr_1_3?ie=UTF8&amp;qid=1484107927&amp;sr=8-3&amp;keywords=code+complete">Pragmatic Programmer</a></li>
<li><a href="https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_2?ie=UTF8&amp;qid=1484107927&amp;sr=8-2&amp;keywords=code+complete">Clean Code</a></li>
<li><a href="https://www.amazon.com/Programming-Language-Brian-W-Kernighan/dp/0131103628/ref=sr_1_1?ie=UTF8&amp;qid=1484108211&amp;sr=8-1&amp;keywords=the+c+programming+language">The C Programming Language</a></li>
<li><a href="https://www.amazon.com/Mastering-Algorithms-Techniques-Sorting-Encryption/dp/1565924533/ref=sr_1_1?ie=UTF8&amp;qid=1484108238&amp;sr=8-1&amp;keywords=mastering+algorithms+in+c">Mastering Algorithms in C (partial)</a></li>
</ul>
</section>
<section id="projects" class="level2">
<h2 class="anchored" data-anchor-id="projects">Projects</h2>
<ul>
<li>Build a workstation</li>
<li>Personal Website</li>
<li>Compete in a Kaggle competition</li>
</ul>
</section>
<section id="coursework" class="level2">
<h2 class="anchored" data-anchor-id="coursework">Coursework</h2>
<ul>
<li><a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Intro to Statistical Learning</a></li>
<li><a href="https://www.edx.org/course/big-data-analysis-apache-spark-uc-berkeleyx-cs110x">Big Data Analysis with Apache Spark</a></li>
<li><a href="https://csmasters.uchicago.edu/page/mpcs-course-catalog#">C Programming</a></li>
</ul>
</section>
<section id="goals" class="level2">
<h2 class="anchored" data-anchor-id="goals">2017 Goals</h2>
</section>
<section id="topics-1" class="level2">
<h2 class="anchored" data-anchor-id="topics-1">Topics</h2>
<ul>
<li>Containers &amp; Docker</li>
<li>Bayesian Statistics</li>
<li>Time Series Analysis</li>
<li>Web programming</li>
<li>Data Pipelines</li>
</ul>
</section>
<section id="books-1" class="level2">
<h2 class="anchored" data-anchor-id="books-1">Books</h2>
<ul>
<li><a href="https://www.amazon.com/Cython-Programmers-Kurt-W-Smith/dp/1491901551/ref=sr_1_1?ie=UTF8&amp;qid=1484107172&amp;sr=8-1&amp;keywords=cython">Cython</a></li>
<li><a href="https://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343/ref=sr_1_1?ie=UTF8&amp;qid=1484107234&amp;sr=8-1&amp;keywords=big+data%3A+principles+and+best+practices">Big Data: Principles and best practices of scalable realtime data systems</a></li>
<li><a href="https://www.amazon.com/Flask-Web-Development-Developing-Applications/dp/1449372627/ref=sr_1_1?ie=UTF8&amp;qid=1484107257&amp;sr=8-1&amp;keywords=flask+web+development">Flask Web Development</a></li>
<li><a href="https://www.amazon.com/Think-Bayes-Bayesian-Statistics-Python/dp/1449370780/ref=sr_1_1?ie=UTF8&amp;qid=1484107459&amp;sr=8-1&amp;keywords=think+bayes">Think Bayes</a></li>
<li><a href="https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?ie=UTF8&amp;qid=1484107411&amp;sr=8-1&amp;keywords=data+analysis+using+regression+and+multilevel+hierarchical+models">Data Analysis Using Regression and Multilevel/Hierarchical Models</a></li>
<li><a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?ie=UTF8&amp;qid=1484107432&amp;sr=8-1&amp;keywords=bayesian+data+analysis">Bayesian Data Analysis</a></li>
</ul>
</section>
<section id="projects-1" class="level2">
<h2 class="anchored" data-anchor-id="projects-1">Projects</h2>
<ul>
<li>Set up home workstation as a server</li>
<li>Contribute to an open source project</li>
<li>Build a docker container image</li>
<li>Compete in Kaggle’s Two Sigma code competition</li>
<li>Build a simple analytics web app</li>
</ul>
</section>
<section id="coursework-1" class="level2">
<h2 class="anchored" data-anchor-id="coursework-1">Coursework</h2>
<ul>
<li><a href="https://www.safaribooksonline.com/library/view/learning-path-scaling/9781491977804/">Scaling Python for Big Data</a></li>
<li><a href="https://www.udacity.com/course/deep-learning--ud730">Deep Learning</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>reflection</category>
  <guid>https://www.zduey.github.io/posts/2016-retrospective-2017-goals.html</guid>
  <pubDate>Tue, 10 Jan 2017 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The Funnel Method</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/funnel-method.html</link>
  <description><![CDATA[ 





<p>A while back, I saw the following post on Twitter about structuring Python scripts.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
A template for <a href="https://twitter.com/hashtag/Python?src=hash">#Python</a> scripting… <a href="https://twitter.com/swcarpentry"><span class="citation" data-cites="swcarpentry">@swcarpentry</span></a> <a href="https://twitter.com/DataScienceHbt"><span class="citation" data-cites="DataScienceHbt">@DataScienceHbt</span></a> <a href="https://t.co/D9DDoU6PrQ">pic.twitter.com/D9DDoU6PrQ</a>
</p>
— Damien Irving (<span class="citation" data-cites="DrClimate">@DrClimate</span>) <a href="https://twitter.com/DrClimate/status/779163490883731456">September 23, 2016</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I really liked the idea of a simple Python template, but I think there are two very easy (and important) improvements to be made. They may seem trivial, but stick with me through the explanation:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Import stuff</span></span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> what_main_func_does(args):</span>
<span id="cb1-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" Call a bunch of functions to do what the script does """</span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># define a bunch of functions</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"__main__"</span>:</span>
<span id="cb1-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># what to do if the script is being executed from the command line (instead of being imported)</span></span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># use the argparse library to handle command line arguments (args)</span></span>
<span id="cb1-12"></span>
<span id="cb1-13">    what_main_does(args)</span></code></pre></div>
<p>The first change you may notice is that I renamed ‘main’ to somethig a bit more descriptive. This may seem nit-picky, but I think it is worth addressing in more detail because I see python files with main() all the time and I think it is a habit we all need to break. Ultimately, it comes down to one very simple fact: Python is not C, i.e.&nbsp;you are not required to have a function called ‘main’ as the entry point to a program. This is a very liberating aspect of the language as it means your top-level function can have a descriptive name keying readers in to what the script is doing.</p>
<p>The other change you will notice is that I put the main function at the top of the script with the function definitions below it. Again, this may seem nit-picky, however, there is a functional (pun intended) reason for doing this that I will get into in a moment. Putting function definitions ahead of your main function is another one of those habits that we all need to break once we have realized that Python is not C. In C, a function declaration must appear in a file before it is used. What this means is that if you wanted to put your ‘main’ function at the top of the script in C, you would have to put all of your function definitions ahead of it. This can start to look quite ugly if you have a lot of functions in the file, so I understand why most C code does not opt for this approach. As an example, consider the top of this C program implementing a hangman game:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb2-1"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;stdbool.h&gt;</span></span>
<span id="cb2-2"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;string.h&gt;</span></span>
<span id="cb2-3"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;stdio.h&gt;</span></span>
<span id="cb2-4"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;stdlib.h&gt;</span></span>
<span id="cb2-5"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;ctype.h&gt;</span></span>
<span id="cb2-6"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">&lt;time.h&gt;</span></span>
<span id="cb2-7"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">#include </span><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">"convert.h"</span></span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bool</span> check_valid_input<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-10"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bool</span> pickword<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> buffer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">size_t</span> maxlength<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-11"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> showdiagram <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">unsigned</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> incorrect<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-12"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">void</span> showguesses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>letter_positions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>guesses<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>misses<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-13"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> readnext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">();</span></span>
<span id="cb2-14"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bool</span> check_valid_input<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-15"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">bool</span> is_valid_guess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> guess<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>guesses<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">);</span></span>
<span id="cb2-16"></span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> main <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">(</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">int</span> argc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">char</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> args<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">)</span></span>
<span id="cb2-19"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb2-20"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">...</span></span>
<span id="cb2-21"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>On the one hand, it is not too hard to just glaze over those definitions, but I can definitely see why having too many more would get distracting and lead you to just put ‘main’ at the end of the file.</p>
<p>Aside from it being possible, there is also a best practices reason for putting your top-level function first. Bear with me in this similie: writing code is like writing an essay. It is good practice to start with an introduction before getting into the heart of the argument. An introduction orients the reader and also gives lazy readers an easy point to tl;dr and move on to something else. The same is true for writing code. Having your high level procedure at the top of the script gives readers of that code (provided you have descriptive function names) a good sense of the script’s purpose. If the reader wants to get into the details, all they need to do is scroll down and continue reading. Keep in mind: this is not a new concept. The idea of having narrative-like code shows up in Kernighan and Plauger’s 1978 book <a href="https://en.wikipedia.org/wiki/The_Elements_of_Programming_Style"><em>The Elements of Programming Style</em></a>.</p>
<p>Robert Martin expands a bit upon this idea in his book <a href="https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?ie=UTF8&amp;qid=1477935335&amp;sr=8-1&amp;keywords=clean+code"><em>Clean Code</em></a> where he describes <em>The Stepdown Rule</em>. At its core, <em>The Stepdown Rule</em> says that each function should descend one level of abstraction such that the code file can be read top to bottom. One way to visualize this principle was to imagine your code file as a funnel (hence the title of this post). The top contains the broadest function, equivalent to Damien’s ‘main’ function in the template. As you get lower in the file, the functions become more and more detailed, all the while, doing exactly one thing. Ideally, what you end up with is code that is self-documenting and that takes the reader on a logical journey from high-level to the nitty-gritty.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Imports</span></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> what_main_does(args):</span>
<span id="cb3-4">    action_1()</span>
<span id="cb3-5">    action_2()</span>
<span id="cb3-6">    action_3()</span>
<span id="cb3-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span></span>
<span id="cb3-8"></span>
<span id="cb3-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> action_1():</span>
<span id="cb3-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># some actions</span></span>
<span id="cb3-11">    _nitty_gritty_routine()</span>
<span id="cb3-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span></span>
<span id="cb3-13">.</span>
<span id="cb3-14">.</span>
<span id="cb3-15">.</span>
<span id="cb3-16"></span>
<span id="cb3-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _nitty_gritty_routine():</span>
<span id="cb3-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># nitty gritty details</span></span>
<span id="cb3-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span></span>
<span id="cb3-20"></span>
<span id="cb3-21"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"__main__"</span>:</span>
<span id="cb3-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># capture command line args</span></span>
<span id="cb3-23">    args <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sys.argv</span>
<span id="cb3-24">    what_main_does(args)</span></code></pre></div>



 ]]></description>
  <category>software development</category>
  <guid>https://www.zduey.github.io/posts/funnel-method.html</guid>
  <pubDate>Sun, 20 Nov 2016 00:00:00 GMT</pubDate>
</item>
<item>
  <title>10,000 Lines of Code</title>
  <dc:creator>Zach Duey</dc:creator>
  <link>https://www.zduey.github.io/posts/10000-lines-of-code.html</link>
  <description><![CDATA[ 





<p>Malcolm Gladwell has this now-famous metric about how 10,000 hours of practice in a sport/trade/etc. is roughly the amount of time required to become a master in that area. Under a certain set of assumptions, this is about 5 years of full-time work. I’ve been working as a pseudo-journeyman programmer for about a year now, so I’m on my way, but I want to establish another somewhat arbitrary milestone: writing your first 10,000 lines of code.</p>
<p>Every Friday, I spend about 20 minutes taking stock of what I did over the course of the past week, update a running document with high-level summaries of projects I’ve worked on or am currently working on, and make a general plan for the week ahead. As part of that routine, I also do a quick linecount of the code I’ve written for various ad-hoc projects, data analytics research, and the small application I build and maintain at work. A few weeks ago this linecount breached the 10,000 mark. Thanks to the publicity-engine and PR aptitude of Gladwell, an immediate bell began going off in my head. My first thought was a bit of shock that in such a short time (around 9 months) I had managed to build up that much code. My second thought turned into a long silent trip down memory lane as I started to reflect on how much I learned – sometimes the hard way – in the course of writing those 10,000 lines.</p>
<section id="start-small-think-big-then-get-back-to-starting-small-i-am-pretty-sure" class="level3">
<h3 class="anchored" data-anchor-id="start-small-think-big-then-get-back-to-starting-small-i-am-pretty-sure">Start small, think big, then get back to starting small I am pretty sure</h3>
<p>that I’ve seen a T-shirt, coffee mug, or meme somewhere with the phrase “Think big, start small”, or some derivation thereof. I want to amend that slightly by adding “start small” to the front of that advice. Perhaps it’s only a certain type of personality, but if you start by thinking big, it can get overwhelming how distant that end goal may be. The best way I’ve found to get around this is to start a project by actually writing some code. Anyone who has had to write papers in school knows the fear of a blinking cursor as you start an essay. Likewise, every seasoned programmer knows that a blank text editor is one of the scarier things you’ll face. However, the second a little code is on the page, things tend to snowball ahead. Now, if you forget to step back and go to step 2 (think big), it’s easy to get caught in that snowballing code base and not think about whether or not where the code is currently going is in fact where you want it to go. After re-acquainting yourself with the bigger picture, it’s time to get back to the details and get that snowball rolling again. Rinse and repeat.</p>
</section>
<section id="read-good-code-ever-wondered-why-the-younger-siblings-always-seem-to-be-the" class="level3">
<h3 class="anchored" data-anchor-id="read-good-code-ever-wondered-why-the-younger-siblings-always-seem-to-be-the">Read good code Ever wondered why the younger siblings always seem to be the</h3>
<p>‘better’ athletes? There is a strong case to be made that it is because they are constantly playing-up, i.e.&nbsp;being mercilessly defeated (or not) by older siblings. This sort of constant challenge pushes them in ways you simply cannot get if you are surrounded by people of similar abilities. I think the same holds true professionally. But, if you are lacking mentorship, turning to the vast swaths of open-source code bases available online can be a fruitful second-best. Digesting that well-written code and borrowing from not only the code style but also the techniques can yield great benefit in your own projects.</p>
</section>
<section id="develop-good-coding-habits-early-sadly-nothing-new-here-again.-bad-habits" class="level3">
<h3 class="anchored" data-anchor-id="develop-good-coding-habits-early-sadly-nothing-new-here-again.-bad-habits">Develop good coding habits early Sadly, nothing new here again. Bad habits</h3>
<p>are hard to break. I remember reading this advice relatively early on in my transition to pseudo-journeyman and it’s kept me honest and relatively anal about code style ever since. Not to mention, once you’ve written a significant amount of code with those bad habits shining through, it is a lot more painful to re-write than if you started with at least some good habits. No idea what constitute ‘good’ coding habits? Here are a couple of resources I’ve found incredibly useful as I’ve started trying to develop my own:</p>
<ol type="1">
<li><a href="https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882/ref=sr_1_1?ie=UTF8&amp;qid=1477935335&amp;sr=8-1&amp;keywords=clean+code">Clean Code</a></li>
<li><a href="https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=sr_1_1?ie=UTF8&amp;qid=1477935229&amp;sr=8-1&amp;keywords=code+complete">Code Complete</a></li>
<li><a href="https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X/ref=sr_1_4?ie=UTF8&amp;qid=1477935335&amp;sr=8-4&amp;keywords=clean+code">Pragmatic Programmer</a></li>
<li><a href="http://docs.python-guide.org/en/latest/">Hitchhiker’s Guide to Python</a></li>
</ol>
</section>
<section id="understand-why-code-works-not-just-that-it-works-it-is-all-too-easy" class="level3">
<h3 class="anchored" data-anchor-id="understand-why-code-works-not-just-that-it-works-it-is-all-too-easy">Understand <em>why</em> code works, not just <strong>that</strong> it works It is all too easy</h3>
<p>to get caught up in the excitement of a snowballing code base. This is especially true when writing code in a high-level language like Python where you have to do much less thinking about the nuts and bolts of a program than if you are writing at a lower level in C/C++ or even a language that enforces OOP like Java. However, at some point, there comes a time where you end up needing to know why something <em>worked</em> and not just that it works. If you have to go back to square one and try to understand your program from the ground up because you got too caught up in adding new features/enhancements/whatever, it is much more painful than if you have been building upon your knowledge of <em>why</em> it works the whole time. In short, if you ever find yourself saying “Wow… I’m not sure how that just worked, but that’s what I wanted it to do.” then stop right there and take the time to understand what is going on. In the best case scenario, the code is right, and you got lucky. Equally likely however, is that the code is wrong, or subtlety wrong, and you’ve just introduced a bug that is going to come back and bite you.</p>
</section>
<section id="learn-the-hard-way-sometimes-it-is-good-for-you-personally-i-learn-best" class="level3">
<h3 class="anchored" data-anchor-id="learn-the-hard-way-sometimes-it-is-good-for-you-personally-i-learn-best">Learn the hard way sometimes: it is good for you Personally, I learn best</h3>
<p>by doing. In my experience, it also seems that this is true for the vast majority of people. Even if you are someone who can ingest a lot of information just by reading or listening, it still seems to be the case that a deeper understanding is gained by actually writing code that does what you’ve been reading about. This does mean that you will occasionally find yourself re-inventing the wheel. If you are re-inventing because you don’t know that wheels exist, then that is a problem. If you are re-inventing it because you need to know how a wheel works to build a better one, then that’s A-Okay. After all this isn’t a new philosophy. School is really just one very long exercise in re-inventing the wheel and it happens all the way through coursework in PhD programs.</p>


</section>

 ]]></description>
  <category>software development</category>
  <guid>https://www.zduey.github.io/posts/10000-lines-of-code.html</guid>
  <pubDate>Mon, 31 Oct 2016 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
