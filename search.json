[
  {
    "objectID": "posts/10000-lines-of-code.html",
    "href": "posts/10000-lines-of-code.html",
    "title": "10,000 Lines of Code",
    "section": "",
    "text": "Malcolm Gladwell has this now-famous metric about how 10,000 hours of practice in a sport/trade/etc. is roughly the amount of time required to become a master in that area. Under a certain set of assumptions, this is about 5 years of full-time work. I’ve been working as a pseudo-journeyman programmer for about a year now, so I’m on my way, but I want to establish another somewhat arbitrary milestone: writing your first 10,000 lines of code.\nEvery Friday, I spend about 20 minutes taking stock of what I did over the course of the past week, update a running document with high-level summaries of projects I’ve worked on or am currently working on, and make a general plan for the week ahead. As part of that routine, I also do a quick linecount of the code I’ve written for various ad-hoc projects, data analytics research, and the small application I build and maintain at work. A few weeks ago this linecount breached the 10,000 mark. Thanks to the publicity-engine and PR aptitude of Gladwell, an immediate bell began going off in my head. My first thought was a bit of shock that in such a short time (around 9 months) I had managed to build up that much code. My second thought turned into a long silent trip down memory lane as I started to reflect on how much I learned – sometimes the hard way – in the course of writing those 10,000 lines.\n\nStart small, think big, then get back to starting small I am pretty sure\nthat I’ve seen a T-shirt, coffee mug, or meme somewhere with the phrase “Think big, start small”, or some derivation thereof. I want to amend that slightly by adding “start small” to the front of that advice. Perhaps it’s only a certain type of personality, but if you start by thinking big, it can get overwhelming how distant that end goal may be. The best way I’ve found to get around this is to start a project by actually writing some code. Anyone who has had to write papers in school knows the fear of a blinking cursor as you start an essay. Likewise, every seasoned programmer knows that a blank text editor is one of the scarier things you’ll face. However, the second a little code is on the page, things tend to snowball ahead. Now, if you forget to step back and go to step 2 (think big), it’s easy to get caught in that snowballing code base and not think about whether or not where the code is currently going is in fact where you want it to go. After re-acquainting yourself with the bigger picture, it’s time to get back to the details and get that snowball rolling again. Rinse and repeat.\n\n\nRead good code Ever wondered why the younger siblings always seem to be the\n‘better’ athletes? There is a strong case to be made that it is because they are constantly playing-up, i.e. being mercilessly defeated (or not) by older siblings. This sort of constant challenge pushes them in ways you simply cannot get if you are surrounded by people of similar abilities. I think the same holds true professionally. But, if you are lacking mentorship, turning to the vast swaths of open-source code bases available online can be a fruitful second-best. Digesting that well-written code and borrowing from not only the code style but also the techniques can yield great benefit in your own projects.\n\n\nDevelop good coding habits early Sadly, nothing new here again. Bad habits\nare hard to break. I remember reading this advice relatively early on in my transition to pseudo-journeyman and it’s kept me honest and relatively anal about code style ever since. Not to mention, once you’ve written a significant amount of code with those bad habits shining through, it is a lot more painful to re-write than if you started with at least some good habits. No idea what constitute ‘good’ coding habits? Here are a couple of resources I’ve found incredibly useful as I’ve started trying to develop my own:\n\nClean Code\nCode Complete\nPragmatic Programmer\nHitchhiker’s Guide to Python\n\n\n\nUnderstand why code works, not just that it works It is all too easy\nto get caught up in the excitement of a snowballing code base. This is especially true when writing code in a high-level language like Python where you have to do much less thinking about the nuts and bolts of a program than if you are writing at a lower level in C/C++ or even a language that enforces OOP like Java. However, at some point, there comes a time where you end up needing to know why something worked and not just that it works. If you have to go back to square one and try to understand your program from the ground up because you got too caught up in adding new features/enhancements/whatever, it is much more painful than if you have been building upon your knowledge of why it works the whole time. In short, if you ever find yourself saying “Wow… I’m not sure how that just worked, but that’s what I wanted it to do.” then stop right there and take the time to understand what is going on. In the best case scenario, the code is right, and you got lucky. Equally likely however, is that the code is wrong, or subtlety wrong, and you’ve just introduced a bug that is going to come back and bite you.\n\n\nLearn the hard way sometimes: it is good for you Personally, I learn best\nby doing. In my experience, it also seems that this is true for the vast majority of people. Even if you are someone who can ingest a lot of information just by reading or listening, it still seems to be the case that a deeper understanding is gained by actually writing code that does what you’ve been reading about. This does mean that you will occasionally find yourself re-inventing the wheel. If you are re-inventing because you don’t know that wheels exist, then that is a problem. If you are re-inventing it because you need to know how a wheel works to build a better one, then that’s A-Okay. After all this isn’t a new philosophy. School is really just one very long exercise in re-inventing the wheel and it happens all the way through coursework in PhD programs."
  },
  {
    "objectID": "posts/intro-bayesian-statistics-presentation.html",
    "href": "posts/intro-bayesian-statistics-presentation.html",
    "title": "Bayesian Statistics – Lunch-and-Learn Presentation",
    "section": "",
    "text": "A few weeks ago, I completed a graduate-level Bayesian Statistics course at Penn (STAT-927). Although I have been interested in Bayesian Statistics for quite a while now, this was the first formal course I have taken. Professionally, I developed some Bayesian multilevel regression models during my time at the Computational Memory Lab as part of the RAM project. However, in my current position, I have not typically leveraged fully Bayesian approaches.\nLast week at work, I gave an overview of Bayesian Statistics during a lunch-and-learn style session. The intention was to have the material be accessible to anyone with some background in probability and at least a high-level understanding of classical statistics (e.g. at least some familiarity with p-values, hypothesis testing, and confidence intervals). The structure of the presentation largely follows the outline from the first couple of lectures from the course (albeit at a slightly higher level), but with the addition of a running example.\nI had a lot of fun putting the slides together and wrestling with how to present the material to a mixed audience (data curators, data scientists, and software engineers). As putting a presentation together often does, it solidified my understanding in many places and pointed to some areas where it is still lacking. I’m hoping to delve into some additional techniques from the course in a future presentation (probably geared towards a smaller audience). In the meantime, here are the slides and code from the initial presentation."
  },
  {
    "objectID": "posts/2017-retrospective-2018-goals.html",
    "href": "posts/2017-retrospective-2018-goals.html",
    "title": "2017 Retrospective and 2018 Goals",
    "section": "",
    "text": "2017 went by in a hurry. My wife and I made the move from Chicago to Philadelphia for her to start a PhD program, which also entailed me switching jobs. After some initial adjustment, work is going well and I have found myself continuing to enjoy writing code for a living. Since switching away from economics research as a career track in 2016, I have been trying to sort out exactly how far towards software engineering I want to veer. My current work is a mixture of model building/validation/testing and more traditional programming projects. Quite honestly, this mixture suits me well, although I often worry about being a jack of all trades and master of none. Although I have no regrets about moving away from the academic research career path, I do wonder whether I’ll need to go back to grad school for people to take my quantitative skills seriously. In the meantime, the plan for 2018 is to continue filling in gaps in my technical background, while adding some more tools to my applied statistics belt."
  },
  {
    "objectID": "posts/2017-retrospective-2018-goals.html#section",
    "href": "posts/2017-retrospective-2018-goals.html#section",
    "title": "2017 Retrospective and 2018 Goals",
    "section": "2018",
    "text": "2018\n\nTopics\n\nOperating Systems\nNetworking\nC++\nWeb Programming\nBayesian Statistics\n\n\n\nBooks\n\nModern Operating Systems\nAccelerated C++\nEffective Modern C++\nBayesian Data Analysis\n\n\n\nProjects\n\nMentorPhilly Web App\nMyRecipeStash Web App\nHomework assignments for Software Systems course\nSubmit PR revising logging in luigi\n\n\n\nCoursework\n\nSoftware Systems"
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html",
    "href": "posts/streaming-stock-data-with-bokeh.html",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "",
    "text": "As part of my 2017 goal to work on a small analytics-oriented web app, I started doing some research into what I would want to use for the visualization component. Being a huge fan of python, I wanted to try out bokeh, which touts interactive visualizations using pure python. Bokeh also allows for a number of different demployment options, including within a Flask app, so it seemed like a reasonable option to consider.\nFor a quick weekend hack, I opted to build a real-time price chart. The Investors Exchange (IEX) recently released an API that allows you to get the last trade price in real-time (and a bunch of other data for that matter) for equities trading on their exchange.\nThe scope of the project was very small: build a single page bokeh app that would stream stock price quotes and allow the user to change which ticker to stream. The script boils down to three components:\n\nA function to get the last traded price\nA callback function to update the ticker being streamed\nThe code to set up the chart"
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html#overview",
    "href": "posts/streaming-stock-data-with-bokeh.html#overview",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "",
    "text": "As part of my 2017 goal to work on a small analytics-oriented web app, I started doing some research into what I would want to use for the visualization component. Being a huge fan of python, I wanted to try out bokeh, which touts interactive visualizations using pure python. Bokeh also allows for a number of different demployment options, including within a Flask app, so it seemed like a reasonable option to consider.\nFor a quick weekend hack, I opted to build a real-time price chart. The Investors Exchange (IEX) recently released an API that allows you to get the last trade price in real-time (and a bunch of other data for that matter) for equities trading on their exchange.\nThe scope of the project was very small: build a single page bokeh app that would stream stock price quotes and allow the user to change which ticker to stream. The script boils down to three components:\n\nA function to get the last traded price\nA callback function to update the ticker being streamed\nThe code to set up the chart"
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html#data",
    "href": "posts/streaming-stock-data-with-bokeh.html#data",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "Data",
    "text": "Data\nThe IEX api has a number of different endpoints. The base url is https://api.iextrading.com/1.0 and the endpoint for accessing the last traded price is /tops. As an example, if you want the last traded price for Snap, Inc. you would send a GET request to: https://api.iextrading.com/1.0/tops?symbols=SNAP. IEX provides very clear documentation, so I won’t go into more detail about usage.\nimport io\nimport requests\nimport pandas as pd\n\n\nbase = \"https://api.iextrading.com/1.0/\"\n\ndef get_last_price(symbol):\n    payload = {\n        \"format\": \"csv\",\n        \"symbols\": symbol\n    }\n    endpoint = \"tops/last\"\n\n    raw = requests.get(base + endpoint, params=payload)\n    raw = io.BytesIO(raw.content)\n    prices_df = pd.read_csv(raw, sep=\",\")\n    prices_df[\"time\"] = pd.to_datetime(prices_df[\"time\"], unit=\"ms\")\n    prices_df[\"display_time\"] = prices_df[\"time\"].dt.strftime(\"%m-%d-%Y %H:%M:%S.%f\")\n\n    return prices_df\nThere are a few things to note about this section. First, I wanted to get the data into a pandas dataframe to do a little post-processing. Using the io library, you can store data in a memory buffer and then read out of that buffer with pandas. This saves you overhead of doing disk i/o. Second, the IEX API returns all times in milliseconds since the Unix epoch. Creating the display_time variable is done in order to have a nicely-formatted date in the tooltip for the chart.\nBokeh has nice integration with pandas.The best option when your data is in a pandas dataframe is to use a ColumnDataSource object, which takes either a dictionary or a pandas dataframe as an argument. I had some trouble with constructing the ColumnDataSource directly from the pandas dataframe because it will include the dataframe’s index as a column. Instead, I went with a slighly clunkier option and explicitly created a dictionary.\ndata = ColumnDataSource(dict(time=[], display_time=[], price=[]))\nOnce the data source is set up, there are a number of methods availabe. However, the only one I will be using is stream() which allows you to append new data to existing columns in your data source.\ndef update_price():\n    new_price = get_last_price(symbol=TICKER)\n    data.stream(dict(time=new_price[\"time\"],\n                     display_time=new_price[\"display_time\"],\n                     price=new_price[\"price\"]), \n                10000)\n    return\nOne thing to note is the second argument to stream(), which controls how many datapoints will be kept in the datasource before rolling off. If you have a large datasource, keeping this parameter reasonably small will keep your browser from getting bogged down in rendering the chart."
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html#updating-the-ticker",
    "href": "posts/streaming-stock-data-with-bokeh.html#updating-the-ticker",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "Updating the Ticker",
    "text": "Updating the Ticker\nYour ColumnDataSource object has a data attribute, which is the dictionary containing the underlying data. Directly modifying this attribute is what allows me to update what ticker is being streamed. When a user submits a new ticker to stream, this attribute is reset and begins receiving data after the next request to the IEX API.\nTICKER = ''\n\ndef update_ticker():\n    global TICKER\n    TICKER = ticker_textbox.value\n    price_plot.title.text = \"IEX Real-Time Price: \" + ticker_textbox.value\n    data.data = dict(time=[], display_time=[], price=[])\n\n    return\nOne thing I am not a fan of is the apparent need to make TICKER a global variable. So far as I can tell, you cannot pass args to your callback functions. It is possible that I am mistaken about this, so I will post an update if I find a better way. The ticker_textbox is the bokeh text input widget. When update_ticker() is called, it uses the current value of that input widget as the new ticker to stream. The funciton also updates the title in the bokeh figure to reflect the currently-streaming ticker."
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html#setting-up-the-dashboard",
    "href": "posts/streaming-stock-data-with-bokeh.html#setting-up-the-dashboard",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "Setting up the Dashboard",
    "text": "Setting up the Dashboard\nThe dashboard is very simple, but there are still a number of components involved. First, to create the tooltip, you do so by creating a HoverTool object.\nhover = HoverTool(tooltips=[\n    (\"Time\", \"@display_time\"),\n    (\"IEX Real-Time Price\", \"@price\")\n    ])\nThe @ syntax corresponds to variables in your data source. Any column in the data can be added to your tooltip. In this case, I am displaying the price and my formatted time variable.\nTo set up the plot, you create a figure object. Many of the figure’s attributes can be configured during creation, or you can access and modify them after the fact. You can see both below:\nprice_plot = figure(plot_width=800,\n                    plot_height=400,\n                    x_axis_type='datetime',\n                    tools=[hover, ResizeTool(), SaveTool()])\n\nprice_plot.line(source=data, x='time', y='price')\nprice_plot.xaxis.axis_label = \"Time\"\nprice_plot.yaxis.axis_label = \"IEX Real-Time Price\"\nprice_plot.title.text = \"IEX Real Time Price: \" + TICKER\nNext, I needed to create two widgets: a textbox for capturing tickers and a button to trigger the update. You can set a callback function for a widget by passing a callable to the on_click()method of a widget. Finally, I bound the two user input widgets together in a widgetbox, which provides the benefit of ensuring all widgets have the same sizing mode.\nticker_textbox = TextInput(placeholder=\"Ticker\")\nupdate = Button(label=\"Update\")\nupdate.on_click(update_ticker)\n\ninputs = widgetbox([ticker_textbox, update], width=200)\nThe last piece is to finish setting up the current document. I arrange the widgetbox and the figure into a single row and bind that to the view. Finally, to stream the data I specify update_price() as the callback function to use at a fixed interval of 1 second (1000ms).\ncurdoc().add_root(row(inputs, price_plot, width=1600))\ncurdoc().add_periodic_callback(update_price, 1000)"
  },
  {
    "objectID": "posts/streaming-stock-data-with-bokeh.html#running-the-app",
    "href": "posts/streaming-stock-data-with-bokeh.html#running-the-app",
    "title": "Streaming Stock Price Data with Bokeh",
    "section": "Running the App",
    "text": "Running the App\nThe full code is available as a gist. Download/clone/etc. the script and then run bokeh serve iex.py from the command line. The bokeh server will fire up and display the dashboard at port 5006. Type in your ticker, hit update, and the price data will begin streaming. Keep in mind that you will only get streaming data when the market is open. If you choose a lightly-traded product, it will be less interesting, so I recommend starting with a big-name firm. For a complete list, IEX provides a regularly-updated list of tickers."
  },
  {
    "objectID": "posts/app-deployment-with-heroku.html",
    "href": "posts/app-deployment-with-heroku.html",
    "title": "Deploying an Application with Heroku",
    "section": "",
    "text": "This is a follow-up to an earlier blog post on building a single-page web app in bokeh. In that post, I went through some of the background on how to build a chart in bokeh that streams data.\nThis post is about deploying an application using as PaaS provider. It is not a thorough comparison of the various PaaS providers out there, because quite honestly, my objective function was this simple:\n\nDo I know about it (binary)\nCan I deploy for free (binary)\nEase of use (hand-wavy continuous variable)\n\nThe providers I was choosing between included:\n\nAmazon AWS\nGoogle Cloud Platform\nDigital Ocean\nHeroku\n\nIn terms of cost, all of the providers (except Digital Ocean) have some sort of free tier. AWS and Google Cloud Platform are both feature-rich, which is both a blessing and a curse. On the plus side, if my intent was to build something scalable and production-ready, those would likely be the clear front-runners. On the negative side, because of all the features, it takes a bit of effort to navigate all of the microservices and to determine what to include as part of the stack. As a hobbyist (at least in this use case), I needed a very small compute instance with a simple deployment mechanism. In the end, Heroku was the top choice because of its free tier and relatively straightforward (and automated) deployment mechanism. The downside to Heroku’s free tier is that after 30 minutes of inactivity, the application will sleep, which causes the page to load a bit slowly the next time it is accessed. Heroku has very readable and helpful documentation, so if I do a poor job with this overview, go directly to the source where they have plenty of information to get you up and running.\nOnce you create your Heroku account, I recommend reading How Heroku Works. It covers all of the concepts you need to get a basic app up and running but without any messy or overly-technical architectural details. After that, work your way through Getting Started on Heroku with Python. As part of the tutorial, you will need to install the Heroku CLI. On my Ubuntu workstation, it was as simple as adding their apt repository. Heroku provides instructions for other operating systems as well.\nsudo add-apt-repository \"deb https://cli-assets.heroku.com/branches/stable/apt ./\" \ncurl -L https://cli-assets.heroku.com/apt/release.key | sudo apt-key add - \nsudo apt-get update\nsudo apt-get install heroku \nWhile you are logged in, go to your dashboard and create a new app. All you need to do is give it a name. The name is not strictly necessary as heroku will randomly choose one if you leave it blank. I named my app iex-streaming. Once you create the app, you will be brought to the deployment screen where you can choose between Heroku Git, GitHub, and Dropbox. I chose to deploy from a GitHub repo. If you go this route, create a repo from within GitHub so that you can connect it to Heroku. Alternatively, you can use Heroku as a remote and push to Heroku directly using the CLI tool. You can also use Dropbox, but I will not talk about that here since I do not see why you would choose that option unless you wanted to avoid having to learn Git. If you are in that boat, then my suggestion would be to rethink that decision and take the time to at least learn the basics of Git. A great way to get started is to go through the Try Git tutorial. This tutorial gives you enough to follow along the rest of the way.\nWith the preliminaries out of the way, there are only a few things that need to be done to get our single page bokeh app deployed. First, turn the folder where your code is stored in a Git repository. You will also need to set up your local repository to track the remote repository. If you are using Heroku Git, the command is a bit different, but you can find the instructions in the deployment page I mentioned earlier.\ngit init\ngit remote add origin https://github.com/zduey/iex.git\nIf you have not already done so, create an environment for the project (I am using conda) and install pandas and bokeh. Once you are done with that, make sure you activate the environment and check that the app still runs locally in case there are any missing dependencies.\nconda create -n iex python=3.6\nsource activate iex\npip install bokeh\npip install pandas\nNow, use pip to export the information about your environment to a requirements.txt file. Heroku will use this file to set up your remote machine when you deploy.\npip freeze &gt; requirements.txt\nNext, create a runtime.txt file. In this file, you will identify the version of python to use for the application. At the time this was written Heroku only supports two python runtimes: python-2.7.13 and python-3.6.0.\ntouch runtime.txt\n# runtime.txt\npython-3.6.0\nIf you would like, you can add a README file.\ntouch README.md\nFinally, you need to create a Procfile. This file contains the instructions for launching your application from Heroku. In the simplest case, it can be a single line long.\ntouch Procfile\n# Procfile\nweb: bokeh serve --port $PORT --host iex-streaming.herokuapp.com --address=0.0.0.0 --use-xheaders iex.py\nThis step was the only part of the deployment where I hit a snag. I ended up relying heavily on this stackoverflow question to work out the last issues I was having.\nStarting on the left, web: is a directive to Heroku indicating the type of process that the command is controlling. In this case, the app requires a single web process. Everything afterwards is the shell command that gets executed to start the app. If you read the earlier post, bokeh serve should be familiar to you. Bokeh comes with a CLI and serve is the command that fires up the bokeh server. Everything else is an argument to this command. --port $PORT --host iex-streaming.herokuapp.com instructs bokeh to listen for requests sent to iex-streaming.herokuapp.com on the port defined by the environment variable $PORT, which is controlled by Heroku. --address=0.0.0.0 tells bokeh to listen on all network interfaces for requests. --use-xheaders is a flag indicating that bokeh should use X-headers for IP/protocol information (according to the man page). Networking is not a strong suit of mine (yet… I’ll get there one day), so unfortunately, that last part is still a black box for me.\nWith all of those files in place, we are just about finished. Your repo should contain the following files:\n\niex.py\nProcfile\nrequirements.txt\nruntime.txt\nREADME.md\n\nGo ahead and add/commit/push. If you are using Heroku Git, your push will be slightly different.\ngit add .\ngit commit -m \"Starter files for app\"\ngit push -u origin master\nAs soon as you push your changes, Heroku will re-deploy your application with the code changes. You can see the build results in your Heroku dashboard and after a minute or two, your changes will be live. Go to your app url and check it out! My sample version is available at:\nhttp://iex-streaming.herokuapp.com/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a father, husband, and data scientist, currently residing in Philadelphia, PA. I am currently the Lead Data Scientist at Activate Care (October 2022).\nProfessionally, I have a dual passion for software engineering and applied statistics with a particular love for all things related to causal inference. At the end of the day, I am most excited by professional and educational opportunities that give me the opportunity to grow in these areas. I completed a master’s degree in computer science at the University of Pennsylvania (May 2022) and received my undergraduate education at American University where I majored in Mathematics & Economics and Philosophy (May 2014).\nMost of my time outside of work is spent with family, friends, and being involved in our local church community (Citylight Church Center City). Since high school, I have cultivated an interest in Christian apologetics, motivated by 1 Peter 3:15-16:\n\n“but in your hearts honor Christ the Lord as holy, always being prepared to make a defense to anyone who asks you for a reason for the hope that is in you; yet do it with gentleness and respect”\n\nI love to cook, make pour-over coffee, and play sports. I was a competitive gymnast for 10 years, dove for a year in high school, and then played ultimate frisbee in college and for a bit afterwards. At this point, I restrict myself to tennis, jogging, and calisthenics to avoid what seemed to be a constant stream of injuries."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Courage to Ask: A Resolution to Counteract Hyperpartisanship\n\n\n\n\n\n\n\nessay\n\n\n\n\nA long-form essay exploring an expected answer to the question of how to counteract hyperpartisanship\n\n\n\n\n\n\nJan 3, 2021\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nEconometrics for Data Scientists\n\n\n\n\n\n\n\npresentations\n\n\ncaual inference\n\n\neconometrics\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2020\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Statistics – Lunch-and-Learn Presentation\n\n\n\n\n\n\n\npresentations\n\n\nbayesian statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2020\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\n2017 Retrospective and 2018 Goals\n\n\n\n\n\n\n\nreflection\n\n\n\n\nA brief reflection on 2017 and goals for 2018\n\n\n\n\n\n\nJan 1, 2018\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nSetting Up SSH On A Home Computer\n\n\n\n\n\n\n\ntutorial\n\n\n\n\nA tutorial for setting up a home computer to be accessed remotely via SSH.\n\n\n\n\n\n\nAug 26, 2017\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nDeploying an Application with Heroku\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\n\n\nSome thoughts on deploying a basic web application with Heroku\n\n\n\n\n\n\nMar 11, 2017\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nStreaming Stock Price Data with Bokeh\n\n\n\n\n\n\n\ntutorial\n\n\npython\n\n\n\n\nTutorial showcasing how to stream stock price data using the bokeh library\n\n\n\n\n\n\nMar 5, 2017\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\n2016 Retrospective and 2017 Goals\n\n\n\n\n\n\n\nreflection\n\n\n\n\nA brief reflection on 2016 and setting goals for 2017.\n\n\n\n\n\n\nJan 10, 2017\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\nThe Funnel Method\n\n\n\n\n\n\n\nsoftware development\n\n\n\n\nThoughts on how to structure a simple python script\n\n\n\n\n\n\nNov 20, 2016\n\n\nZach Duey\n\n\n\n\n\n\n  \n\n\n\n\n10,000 Lines of Code\n\n\n\n\n\n\n\nsoftware development\n\n\n\n\nA short reflection on software development after writing my first 10,000 lines of code\n\n\n\n\n\n\nOct 31, 2016\n\n\nZach Duey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a holding ground for my personal blog, projects, and other resources. If history is a good predictor of the future, this will remain lightly populated. I tend to prefer reading, face-to-face conversations, and generally anything that gets me away from a screen."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html",
    "href": "posts/ssh-server-on-home-computer.html",
    "title": "Setting Up SSH On A Home Computer",
    "section": "",
    "text": "tl;dr – The guide I wish I had when setting up an SSH server on a home pc"
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#background",
    "href": "posts/ssh-server-on-home-computer.html#background",
    "title": "Setting Up SSH On A Home Computer",
    "section": "Background",
    "text": "Background\nAround 18 months ago, I built a desktop computer so I would have a little more firepower at my fingertips when I needed it. My hope was to avoid having to pay for cloud computing services during Kaggle competitions and other side projects. Early on, I realized that it would be nice to be able to use this machine remotely, so I found a few resources for setting up your home PC as an SSH server. However, I was not able to find a single resource that provided enough background information that I was not just copy/pasting the commands.\nAlthough no advanced knowledge of any particular topic is required to set up your own SSH server, there are many concepts to get your head around. I will explain more detail below, but the short version is that an SSH server is a process running on your computer that waits for outside computers to request access via a specific port, authenticates that user, and then allows access to the computer. If half of those terms do not make sense, do not worry. I was in the same position when I first tried getting set up. Here is a quick list of the concepts/terms that we will cover:\n\nServer\nSSH\nNetwork Address Translation\nPublic/Private IP Addresses\nPorts\nPort Forwarding\nPublic/Private Key Authentication\n\nI use Ubuntu 14.04 LTS as my operating system, however, the steps will be similar on any unix-like platform. To follow along, be sure to have access to the following:\n\nThe unix-based computer that will host the SSH server\nA different computer to test the remote connection to the server\nAccess to a different network than the one to which your host machine is connected (optional, but recommended)\n\nFor the purposes of this guide, ‘host’ will indicate the computer running the SSH server, while ‘client’ will refer to any computer requesting access to the host. At one point in this guide, your host computer will also be a client."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#initial-setup",
    "href": "posts/ssh-server-on-home-computer.html#initial-setup",
    "title": "Setting Up SSH On A Home Computer",
    "section": "Initial Setup",
    "text": "Initial Setup\nSSH stands for “secure shell” and is what will allow us to establish a secure connection between two computers. Our end goal is to be able to issue commands from a client machine that are executed by the host machine. For a more thorough coverage of SSH, take a look at this great guide by Digital Ocean.\nThe term “server” is often used a bit loosely. Here, we mean by the term is a process running on a computer that is tasked with managing access to a computer’s resources over a network. We will be installing the openssh-server application, which will allow us to run an SSH server on our machine that will handle requests for access to the host computer from other devices.\nStart by updating your system packages:\nsudo apt-get upgrade\nInstall the openssh-server application and client. You should also install the openssh-client on machines that will be used as clients.\nsudo apt-get install openssh-client\nsudo apt-get install openssh-server\nYou should now have an SSH server process running on your machine. Check with the following:\nps -A | grep sshd\nYou should see something like:\n[number] ?  00:00:00 sshd\nFor those of you who copy/pasted the above command (something I have certainly been guilty of), you will learn a lot more by taking the time to look up any commands you do not recognize. If you are new (or just want a refresher on) to shell commands, check out the Learn the Command Line course from Code Academy.\nCheck that you can ‘login’ to the host machine from the host machine, i.e. your host machine is also the client in this case.\nssh localhost\nMove around a little in the shell (cd, pwd, etc.) to make sure that everything works and then type “exit” to end the session.\nNext, we will connect using a true ‘client’ machine. For this to work, make sure that your client machine is on the same network as the server. If you are following along from your home or work wifi network, you should be all set.\nStart by determining the IP address of your host machine by listing information about the network interfaces on your machine.\nifconfig\nYou should see entries for ‘eth0’ and ‘lo’, as well as some other entries if you have installed a wireless card on your desktop. If your host machine is connected to the network via an ethernet cable, use the address associated with the “eth[number]” entry. If you are connected via wifi, use the “wlan[number]” entry. If you want to see just the list of IP addresses for the devices, you can use:\nifconfig | grep \"inet addr\"\nNext, try logging in from the client machine using that IP address:\nssh username@X.X.X.X\nYou should be prompted for the password of the account you used in your ssh command. For security reasons, the characters will not be displayed in the shell. If successful, you should see a “Welcome to Ubuntu X.X.X” message or something similar depending on your OS.\nThis is a bit more useful than our previous test, but it still does not solve our initial problem of being able to access the machine from outside the same network. Go ahead and try it out if you have the ability to connect your client machine to a different network than your host machine.\nA quick option if you have a little data to spare on your cellular plan is to turn on tethering and connect your client machine to that network, while your host machine remains on the other network. Once you are on this separate network, try that ssh command again:\nssh username@X.X.X.X\nMost likely, you will notice that the command hangs and then eventually times out. Go ahead and disconnect from this other network and we will finish with the last step of the setup. But first, we will take a second to cover a few more background concepts that will make this next step clearer."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#nat-and-publicprivate-ip-addresses",
    "href": "posts/ssh-server-on-home-computer.html#nat-and-publicprivate-ip-addresses",
    "title": "Setting Up SSH On A Home Computer",
    "section": "NAT and Public/Private IP Addresses",
    "text": "NAT and Public/Private IP Addresses\nIt may be surprising, but the internet is only able to handle a finite number of users being on line at a single time. In a world where almost every electronic device has access to the internet, it is possible to get close to the theoretical maximum. In order to circumvent this problem, Network Address Translation (NAT) was created to allow a set of devices on a private network to share a single IP address. On the private network, each device receives its own private IP address. For example, the IP address you found above with “ifconfig” is the private IP address of your computer on your local network.\nAny time you acess a web page from one of the devices on your home network, the request is routed (hence the term router) through an NAT device, which translates the private IP address into a request using the public IP address assigned to the router by your ISP. This is the IP address you will ultimately need to use when connecting from outside your local network. Go ahead and determine your public IP address by going to www.whatsmyip.org. You should see something in the form of XX.XX.XXX.XXX. You may be tempted to try that ssh command again with this address, but in all likelihood it still will not work.\nAlthough NAT solves the device-limit problem, it adds a layer of complexity to setting up a home computer to accept SSH connections. When a client machine sends a request to connect to the public IP address, your router does not know which of the devices on your private network the request is meant for. Luckily, the solution is usually straightforward and involves setting up ‘port fowarding’ on your router. Bear with me as we continue down the terminology rabbit hole and go over ports. Feel free to skip ahead if you are already familiar with the concept."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#ports-and-port-fowarding",
    "href": "posts/ssh-server-on-home-computer.html#ports-and-port-fowarding",
    "title": "Setting Up SSH On A Home Computer",
    "section": "Ports and Port Fowarding",
    "text": "Ports and Port Fowarding\nAlthough computers have a variety of physical ports that most people are familiar with (USB, HDMI, VGA, etc.), the ports I am referring to here are networking ports and are logical, not physical. When you start a server (i.e. the SSH server you just set up in the previous steps), you bind it to one of these logical ports. The process then ‘listens’ for messages sent to this port by other programs (either internal or external). Each port has a number and some of these numbers are reserved for use by specific types of services. As an example, servers handling HTTP request use port 80 as the default. When you access a web page via a URL, you are really sending a message to an HTTP server at port 80 for a specific resource (web page or other content) that is managed by that service. In general, port 22 is used for the SSH service.\nIn this context, port forwarding means telling your router to forward requests made using a specific port to a particular device on your private network. It is then the responsibility of that device to handle the request. You may be wondering: why don’t just send the request directly to the computer via the private IP address and avoid all of this port forwarding nonsense? Like the name implies, these IP addresses are private, so once you are on a different network, the internet-at-large has no knowledge of this private IP address and therefore the request to connect will fail.\nEvery router is a little bit different when it comes to setting up port forwarding. This may sound like a cop out, but honestly, your best bet is to look up directions for your specific router. For example, here are instructions for an AT&T router and Comcast XFINITY. In general, you will need to take the following actions:\n\nLog in to your router’s admin page\nNavgiate to the page for adding a service (SSH is usually one of the default options)\nSelect or enter the port number where requests will be made (22 by default for SSH)\nSelect or input the private IP address you found earlier of your host machine\nSave the updated settings\n\nNow, we can repeat our ssh command using the public IP address and that request should be redirected by our router to our host machine. Connect back to that outside network and try:\nssh username@[public IP address]\nYou should again be prompted for your host machine password and then admitted access."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#more-secure-ssh-server-configuration",
    "href": "posts/ssh-server-on-home-computer.html#more-secure-ssh-server-configuration",
    "title": "Setting Up SSH On A Home Computer",
    "section": "More Secure SSH Server Configuration",
    "text": "More Secure SSH Server Configuration\nFor this final section, we will be making some changes to the configuration settings for our SSH server. On Ubuntu, these settings are located in the /etc/ssh/sshd_config file. A couple of quick notes:\n\nTo edit this file, you will need to open it as a super user\nAny time you update the config settings, you need to restart the SSH server to have them take effect\n\nsudo restart ssh\nWhen a running SSH server receives a request over port 22 (or whatever port it is bound to), it must check that the requesting user has permission to access the computer. This can be done with a password, however, it is considered much more secure to use public key authentication. Public key cryptography works by having a user generate a public key that can be given to anyone, while keeping the private key hidden. Authentication works by using the public key (made available by the user to the process doing the authentication) to verify that the requesting user (whose request to access the service is encrypted using their private key) matches a user who is allowed access. Once this connection is established, more efficient methods of data encryption/decryption are used to transfer the keystrokes and other information using this secure connection.\nIf you have not already done so, you will need to generate a public/private key pair to use when logging in to your machine. Note that you will need to do this on any device you wish to be allowed access to your SSH server. There are a number of good guides out there for doing this, so to avoid re-inventing the wheel, I suggest following the Ubuntu Instructions\nNow that you have those keys ready, we will enable public key authentication on the SSH server. Open the sshd_config file as a super user for the next set of steps and start by changing:\nPasswordAuthentication yes\nto\nPasswordAuthentication no\nThis will prevent users from being able to access the server with a password, which will help against a brute force attack that attempts to guess the password. Another recommendation is to change the port that the SSH service uses. Within your config file, comment out the existing port specification and choose a new port.\n# Port 22\nPort [new port number]\nRemember, based on an earlier step, your router will still attempt to send SSH requests to port 22, so you will need to go back into your router’s config settings and update the port number for the SSH service. If you are on a machine with multiple user accounts, you can also limit which users are allowed to log in through SSH. At the bottom of the config file add:\nAllowUsers [user1] [user 2] ...\nYou can also deny specific users and add/deny groups, however, it is unlikely that you will need to do this for a home computer.\nNext, disable root login via SSH by changing:\nPermitRootLogin without-password\nto\nPermitRootLogin No\nBeyond disabling password logins, you can also prevent brute force attacks by limiting the number of concurrent connections to the SSH server. For a home computer, keep this number low by changing:\n#MaxStartups 10:30:60\nto\nMaxStartups 3\nWith an SSH connection, you can tunnel graphical windows. If you do not plan to use this ability,then go ahead and shut it off by changing\nX11Forwarding yes\nto\nX11Forwarding no\nIn addition to forwarding windows, you can also forward ports via SSH. For example, if you had a web server running on port 80 on your host machine, you could forward that port to a port on your client machine. This can be handy, but if you do not plan on using this ability, then disable it until you have a need for it since it can be used maliciously.\nAllowTcpForwarding no\nThere are a bunch of other settings in the ssh_config file, but I do not recommend changing the defaults unless you have a good reason to do so. While the suggested changes will add some security to your server, there are many additional ways to add security. In fact, they probably deserve a post of their own, but I will save that for another time.\nThanks for taking the time to read through this and please feel free to let me know if anything is unclear."
  },
  {
    "objectID": "posts/ssh-server-on-home-computer.html#external-resources",
    "href": "posts/ssh-server-on-home-computer.html#external-resources",
    "title": "Setting Up SSH On A Home Computer",
    "section": "External Resources",
    "text": "External Resources\n\nDigital Ocean SSH Essentials\nUbuntu SSH Configuration Guide\nUbuntu Public/Private Keys Guide\nSecure SSH Configuration"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html",
    "href": "posts/2016-retrospective-2017-goals.html",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "",
    "text": "2016 was a pretty good year. I transitioned away from an earlier stage of work life where I thought economics research was all I wanted to be doing. As it turns out, seeing how the research sausage gets made can be just as ugly as seeing how any other sausage gets made. After 18 months as a research assistant, I took a new job with more of a software development bent. For 2016 and again for 2017, I’ve tried picking out a few areas that I want to focus on in my reading, side projects, etc. Otherwise, I end up with way too much breadth and not enough depth. I’m probably still too far on the breadth side, but hey, there is always 2018…"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#topics",
    "href": "posts/2016-retrospective-2017-goals.html#topics",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Topics",
    "text": "Topics\n\nCoding best practices\nStatistical/Machine Learning\nGUI Programming"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#books",
    "href": "posts/2016-retrospective-2017-goals.html#books",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Books",
    "text": "Books\n\nCode Complete\nPragmatic Programmer\nClean Code\nThe C Programming Language\nMastering Algorithms in C (partial)"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#projects",
    "href": "posts/2016-retrospective-2017-goals.html#projects",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Projects",
    "text": "Projects\n\nBuild a workstation\nPersonal Website\nCompete in a Kaggle competition"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#coursework",
    "href": "posts/2016-retrospective-2017-goals.html#coursework",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Coursework",
    "text": "Coursework\n\nIntro to Statistical Learning\nBig Data Analysis with Apache Spark\nC Programming"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#goals",
    "href": "posts/2016-retrospective-2017-goals.html#goals",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "2017 Goals",
    "text": "2017 Goals"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#topics-1",
    "href": "posts/2016-retrospective-2017-goals.html#topics-1",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Topics",
    "text": "Topics\n\nContainers & Docker\nBayesian Statistics\nTime Series Analysis\nWeb programming\nData Pipelines"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#books-1",
    "href": "posts/2016-retrospective-2017-goals.html#books-1",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Books",
    "text": "Books\n\nCython\nBig Data: Principles and best practices of scalable realtime data systems\nFlask Web Development\nThink Bayes\nData Analysis Using Regression and Multilevel/Hierarchical Models\nBayesian Data Analysis"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#projects-1",
    "href": "posts/2016-retrospective-2017-goals.html#projects-1",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Projects",
    "text": "Projects\n\nSet up home workstation as a server\nContribute to an open source project\nBuild a docker container image\nCompete in Kaggle’s Two Sigma code competition\nBuild a simple analytics web app"
  },
  {
    "objectID": "posts/2016-retrospective-2017-goals.html#coursework-1",
    "href": "posts/2016-retrospective-2017-goals.html#coursework-1",
    "title": "2016 Retrospective and 2017 Goals",
    "section": "Coursework",
    "text": "Coursework\n\nScaling Python for Big Data\nDeep Learning"
  },
  {
    "objectID": "posts/funnel-method.html",
    "href": "posts/funnel-method.html",
    "title": "The Funnel Method",
    "section": "",
    "text": "A while back, I saw the following post on Twitter about structuring Python scripts.\n\n\nA template for #Python scripting… @swcarpentry @DataScienceHbt pic.twitter.com/D9DDoU6PrQ\n\n— Damien Irving (@DrClimate) September 23, 2016\n\n\nI really liked the idea of a simple Python template, but I think there are two very easy (and important) improvements to be made. They may seem trivial, but stick with me through the explanation:\n# Import stuff\n\ndef what_main_func_does(args):\n    \"\"\" Call a bunch of functions to do what the script does \"\"\"\n\n# define a bunch of functions\n\nif __name__ == \"__main__\":\n    # what to do if the script is being executed from the command line (instead of being imported)\n\n    # use the argparse library to handle command line arguments (args)\n\n    what_main_does(args)\nThe first change you may notice is that I renamed ‘main’ to somethig a bit more descriptive. This may seem nit-picky, but I think it is worth addressing in more detail because I see python files with main() all the time and I think it is a habit we all need to break. Ultimately, it comes down to one very simple fact: Python is not C, i.e. you are not required to have a function called ‘main’ as the entry point to a program. This is a very liberating aspect of the language as it means your top-level function can have a descriptive name keying readers in to what the script is doing.\nThe other change you will notice is that I put the main function at the top of the script with the function definitions below it. Again, this may seem nit-picky, however, there is a functional (pun intended) reason for doing this that I will get into in a moment. Putting function definitions ahead of your main function is another one of those habits that we all need to break once we have realized that Python is not C. In C, a function declaration must appear in a file before it is used. What this means is that if you wanted to put your ‘main’ function at the top of the script in C, you would have to put all of your function definitions ahead of it. This can start to look quite ugly if you have a lot of functions in the file, so I understand why most C code does not opt for this approach. As an example, consider the top of this C program implementing a hangman game:\n#include &lt;stdbool.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;ctype.h&gt;\n#include &lt;time.h&gt;\n#include \"convert.h\"\n\nbool check_valid_input(char c);\nbool pickword(char * buffer, size_t maxlength);\nvoid showdiagram (unsigned int incorrect);\nvoid showguesses (char *letter_positions, char *guesses, char *misses);\nchar readnext();\nbool check_valid_input(char c);\nbool is_valid_guess(char guess, char *guesses);\n\n\nint main (int argc, char ** args)\n{\n...\n}\nOn the one hand, it is not too hard to just glaze over those definitions, but I can definitely see why having too many more would get distracting and lead you to just put ‘main’ at the end of the file.\nAside from it being possible, there is also a best practices reason for putting your top-level function first. Bear with me in this similie: writing code is like writing an essay. It is good practice to start with an introduction before getting into the heart of the argument. An introduction orients the reader and also gives lazy readers an easy point to tl;dr and move on to something else. The same is true for writing code. Having your high level procedure at the top of the script gives readers of that code (provided you have descriptive function names) a good sense of the script’s purpose. If the reader wants to get into the details, all they need to do is scroll down and continue reading. Keep in mind: this is not a new concept. The idea of having narrative-like code shows up in Kernighan and Plauger’s 1978 book The Elements of Programming Style.\nRobert Martin expands a bit upon this idea in his book Clean Code where he describes The Stepdown Rule. At its core, The Stepdown Rule says that each function should descend one level of abstraction such that the code file can be read top to bottom. One way to visualize this principle was to imagine your code file as a funnel (hence the title of this post). The top contains the broadest function, equivalent to Damien’s ‘main’ function in the template. As you get lower in the file, the functions become more and more detailed, all the while, doing exactly one thing. Ideally, what you end up with is code that is self-documenting and that takes the reader on a logical journey from high-level to the nitty-gritty.\n# Imports\n\ndef what_main_does(args):\n    action_1()\n    action_2()\n    action_3()\n    return\n\ndef action_1():\n    # some actions\n    _nitty_gritty_routine()\n    return\n.\n.\n.\n\ndef _nitty_gritty_routine():\n    # nitty gritty details\n    return\n\nif __name__ == \"__main__\":\n    # capture command line args\n    args = sys.argv\n    what_main_does(args)"
  },
  {
    "objectID": "posts/econometrics-for-data-scientists.html",
    "href": "posts/econometrics-for-data-scientists.html",
    "title": "Econometrics for Data Scientists",
    "section": "",
    "text": "One of my favorite parts about being in an R&D group is that we have a periodic “Journal Club” where we all read a paper that is relevant to some aspect of projects we are working on, and then discuss. Last year, we were reading some papers about factors that contribute to Parkinson’s Disease (PD) progression. In some cases, the authors were leveraging observational data to make claims (often using causal language) about the relationship between the factors they were studying and the rate of disease PD progression. Given my educational and early professional background in econometrics, I read those papers expecting there to be a focus on establishing and defending an “identification strategy” – the combination of theory and methodology that help justify a causal claim. To my surprise, these papers neither made such an effort nor leveraged the methods that I expected: difference in differences, instrumental variables, regression discontinuity, etc.\nIn talking about these papers with my co-workers, it was clear that I was in the minority with my surprise at the lack of an identification strategy. This sparked the idea to put together some materials that I tentatively titled “Econometrics for Data Scientists.” Around the same time, I was also finishing up a course in Statistical Learning at Penn (STAT-974). The course emphasized the algorithmic nature of inferential methods and largely de-emphasized causal inference as an enterprise. However, towards the end of the course, we covered a technique called “double ML” – a method for leveraging machine learning techniques for doing causal inference. The timing seemed too perfect to pass up: over the next few weeks I put together a series of Jupyter notebooks and slides covering some core econometric methods all the way through to some more recent methods that bridge the machine learning/causal inference divide. For anyone looking to learn more, the slides contain numerous links (organized by topic) to other useful books, articles, and blog posts."
  },
  {
    "objectID": "posts/courage-to-ask.html",
    "href": "posts/courage-to-ask.html",
    "title": "Courage to Ask: A Resolution to Counteract Hyperpartisanship",
    "section": "",
    "text": "I originally published this essay on Medium. While the framing (2021 resolutions) is now outdated, the issue remains as do the original conclusions."
  },
  {
    "objectID": "posts/courage-to-ask.html#prior",
    "href": "posts/courage-to-ask.html#prior",
    "title": "Courage to Ask: A Resolution to Counteract Hyperpartisanship",
    "section": "Prior",
    "text": "Prior\nIn Bayesian statistics, a prior is a way of encoding existing knowledge before observing data. The corresponding notion in the framework is quite similar. Our preexisting beliefs inevitably come into play when we assess new pieces of information. Importantly, these beliefs consist of not only our background knowledge but also our past experiences and biases. In our marble-picking example, we had no background knowledge about the bag’s contents and therefore selected a flat prior, which treated all bag configurations as equally likely. The underlying principle was that we wanted to select a prior that was consistent with what we knew about the bag’s contents. This same principle applies to the framework; we want to incorporate what we know about a topic when assessing new information. In Bayesian statistics, the prior is something that we explicitly choose. Similarly, we have some influence over the role that our preexisting beliefs play when assessing new information.\nIn Bayesian statistics, priors fall along a spectrum from non-informative to informative. A non-informative prior is loosely defined as one that has a minimal impact on posterior inferences (updated beliefs in the framework). As a result, these posterior inferences are largely driven by data and the likelihood. A flat prior is at the furthest end of the non-informative side of the spectrum. In the framework, a flat prior is analogous to having no preexisting beliefs, such that our updated beliefs are formed exclusively by new information in conjunction with our ideologies. This approach may seem reasonable when we form opinions about topics with which we are unfamiliar. However, in Bayesian statistics—as in opinion formation—completely non-informative priors can lead to posterior distributions that are not useful when making decisions. This situation arises for one of two reasons: either there is little data available, or the data that is available is highly variable. As an example of the first case, consider what would happen in the marble example if we were only allowed to select a single marble. We know that it would be blue or white, which would help to rule out either the all-white or all-blue options; other than that, we would have learned little about what the bag contains. The second situation, having highly variable data, is one that most parents have probably encountered. There is a plethora of competing parenting information available, such that reading all the available information is unlikely to result in any clear conclusions. In Bayesian statistics, one way to avoid these pitfalls is to replace the flat, non-informative prior with a more informative one.\nA weakly informative prior plays a supporting role in influencing the posterior distribution. In the framework, this type of prior is analogous to allowing our background knowledge to have some influence on our updated beliefs. In the marble scenario, imagine that we are told that bags with equal numbers of blue and white marbles are produced more frequently than other bags. This information is useful, albeit limited. On the one hand, we could ignore it, but then we are no better off than we were before we had this information. A weakly informative prior in this situation is one that assigns, for example, a 4% higher probability to the 2 blue and 2 white marbles bag and therefore a 1% lower probability to the remaining four options. This example exposes an important question that arises in Bayesian statistics: how informative should the prior be? The corresponding question in the framework is: how much should we allow our preexisting beliefs to influence our opinions? Before we try to answer this question directly, we will first consider what happens if we select a prior that is further on the informative end of the spectrum.\nIn the framework, an informative prior is one that has an outsized impact on how our opinion changes (or does not change) in the presence of new information. The analogue in Bayesian statistics is a prior that assigns a high degree of probability to a narrow range of possible values. Imagine that we assigned a 60% probability to the bag with 2 blue and 2 white marbles and a 10% probability to the remaining four options. Although the data (blue, blue, white) remains the same, the resulting posterior probabilities here will be much closer to our prior probabilities than they were when we used a flat prior. In this situation, we assigned the prior probabilities somewhat arbitrarily based on vague information from the manufacturer. However, this informative prior would be reasonable if the manufacturer had said, “More than half of all bags produced have equal numbers of blue and white marbles,” rather than that these types of bags are produced “more frequently.” Regardless of the phrasing, it would take an incredible amount of marble-picking to diverge from these prior beliefs. Likewise, in the framework, if we have strong preexisting beliefs, it will take an incredible amount of new information to change our minds. There is nothing inherently wrong with allowing our preexisting beliefs to influence our updated beliefs, but it is necessary for us to accurately assess the degree to which these beliefs are informed by our past experiences and biases rather than our prior knowledge.\nThere can be more than one valid way to choose a prior in both Bayesian statistics and the framework. All priors fall somewhere along the non-informative/informative spectrum and there are many reasonable options. As a general principle, we can say that the less arbitrary the prior, the better. Practically speaking, applying this principle in the framework means taking the time to first understand where along the spectrum our preexisting beliefs fall and what is driving them. Are we so open to new information that we ignore what we already know? Or conversely, do we hold our beliefs so strongly that no amount of new information can change our minds? If we answer “yes” to the second question, then we must dig deeper to understand where these beliefs originated. In going through this introspective exercise, it will likely become clear how difficult it is to consistently avoid both of these extremes. Armed with this self-knowledge, we should feel a stronger sense of empathy for others who are undoubtedly struggling to strike the same balance. If we are committed to fixing the problem of hyperpartisanship, we must all be willing to do the hard work of both practicing self-reflection to understand and correct our own priors and approaching others with empathy."
  },
  {
    "objectID": "posts/courage-to-ask.html#likelihood",
    "href": "posts/courage-to-ask.html#likelihood",
    "title": "Courage to Ask: A Resolution to Counteract Hyperpartisanship",
    "section": "Likelihood",
    "text": "Likelihood\nIn Bayesian statistics, the likelihood is a function that tells us how likely we are to observe some data given a particular set of values for the unknown parameters. In other words, it is the mechanism through which data influences the posterior distribution. In the marble-picking example, the unknown parameter is the bag’s contents and the likelihood is the count of the number of ways the observed data could have occurred if a particular conjecture about the bag’s contents was true. The analogue in the Bayesian framework is an ideology, or belief system, through which we interpret new information. Just as the likelihood is fundamental to Bayesian statistics, so are the ideologies that we all, either implicitly or explicitly, bring to the table. Our ideologies may not be something that we actively ponder, but they nonetheless mediate how we interpret information.4 Ideologies come in many forms, are not mutually exclusive, and are not necessarily political in nature. For the purpose of this essay, if it ends in “-ism,” it is probably an ideology; liberalism, conservatism, libertarianism, federalism, stoicism, capitalism, and fascism are all ideologies.\nTwo individuals thinking critically about the same issue may come to wildly different conclusions because of their differing ideologies. This observation is grounded in Bayesian statistics; if the priors and data are identical, but are passed through two different likelihoods, the posterior distributions will be different. Similarly, two individuals with identical preexisting beliefs who are exposed to the same new information will come to different conclusions if they have different ideologies. Ideologies are an integral part of the process of opinion formation, and they are not inherently good or bad. Instead, just as an informative prior is only problematic when it overshadows relevant information, ideology only becomes a problem when it plays an outsized role in how we update our opinions. In Bayesian statistics, the likelihood is the dominant factor when there is a large amount of data. Analogously, our ideologies have a much greater influence on our opinions when we process a lot of information. This observation suggests that in a world where all individuals are incorporating all available information, we can expect our opinions to diverge along the same lines as our underlying ideologies. If we are aware of this potential outcome, we should hesitate to attribute someone’s conflicting view to that person’s failure to accurately assess the available information. Ultimately, it is important for us to be transparent about our ideological differences so that when we inevitably reach different conclusions, we can determine the root cause."
  },
  {
    "objectID": "posts/courage-to-ask.html#data",
    "href": "posts/courage-to-ask.html#data",
    "title": "Courage to Ask: A Resolution to Counteract Hyperpartisanship",
    "section": "Data",
    "text": "Data\nThe role that data plays in Bayesian statistics is equivalent to the role of new information in the framework. Just as information is an integral part of every decision we make, any statistical analysis requires data. If there is no data, then the likelihood becomes unusable, and we are left with only the prior. The prior is sufficient for making inferences, but these inferences would be much better if they were informed by data. While ignoring all data may sound extreme, we run into the same problems when we only incorporate some data. To make things concrete, consider how the posterior probabilities would change in our marble example if we decided to ignore the third draw (blue, blue, rather than blue, blue, white). We would no longer assign zero probability to the conjecture that the bag contains no white marbles. As a result, there is more uncertainty about the bag’s contents because we must consider the possibility that the bag contains all blue marbles. A nearly identical parallel exists in the framework. Without new information, we must rely entirely on our preexisting beliefs to make decisions; if we only incorporate some relevant information, our updated opinions still rely heavily on our preexisting beliefs.\nHere we encounter another problem. When multiple individuals choose to incorporate only a subset of the available data, they will often come to different conclusions. In Bayesian statistics, if the priors and likelihoods from two analyses are the same, the posterior distributions are different only if the data is different. Leveraging the marble-picking example again, consider what might happen if a different individual—call him Jack—who has the same background knowledge as Jill also draws three marbles from the bag. If Jack draws three white marbles in a row, he will come to a different conclusion about what the bag likely contains, despite the fact that both he and Jill drew the same number of marbles from the same bag. A particularly stark contrast would develop between their beliefs about whether or not the bag contains all white marbles. Having seen two blue marbles, Jill would vehemently deny the possibility that the bag contains all white marbles, whereas Jack would consider it to be a plausible option. Of course, one obvious solution to this problem would be for Jack and Jill to share their information with each other. We must be similarly open to sharing information when we find ourselves at odds with one another, because even if we are making every effort to critically evaluate the information we access, the mere fact that we access different sources of information can lead us to have divergent viewpoints. Unless we take the time to discuss, we can only make assumptions about the sources of our disagreements and we are left with an incomplete understanding of the situation.\nAt this point, we have seen how changes to any of the three components of Bayesian statistics (prior, likelihood, and data) result in different posterior distributions. Similarly, when individuals form opinions, differences in preexisting beliefs, ideologies, or information result in divergent beliefs. More importantly, in each instance, there is a reasonable explanation for why two individuals who are thinking critically can come to different conclusions. In order to resolve, or at least understand, these differences, we need to do two things: honestly reflect on what is driving our own opinions and deliberately approach others with empathy in order to understand what is driving their opinions."
  },
  {
    "objectID": "posts/courage-to-ask.html#footnotes",
    "href": "posts/courage-to-ask.html#footnotes",
    "title": "Courage to Ask: A Resolution to Counteract Hyperpartisanship",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have limited my scope in this way based on the premise that each of us, to varying degrees, has the ability to choose how we respond to the incentives and rules promulgated by the institutions in which we participate and the systems that they compose. If we accept this premise, then it follows that many of the properties of our environments are emergent phenomena, therefore we do ourselves a disservice by operating as though “the system” has agency. I will not explore this idea further in this essay, which I expect to be immensely unsatisfying for some. However, I hope that the ideas that I present will be sufficiently thought-provoking that it is worth the time investment to continue reading, even if you reject this premise.↩︎\nThere is much more that can and should be said about the role of the media in increasing polarization, but, again, this topic is beyond the scope of this essay. ↩︎\nIn this section, I lean heavily on Richard McElreath’s excellent book, Statistical Rethinking, which provides a great introduction to this topic.↩︎\nThe irony is that this statement is itself ideologically-grounded and therefore not one that I expect to be shared by every reader. The underlying premise is that complete objectivity is an unattainable ideal, and it is therefore better to aim to be aware of our ideological baggage rather than to eliminate it.↩︎"
  }
]